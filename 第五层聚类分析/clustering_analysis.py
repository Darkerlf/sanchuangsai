"""
å¨åˆ€å¸‚åœºèšç±»åˆ†æ - ä¼˜åŒ–ç‰ˆ v2.0
================================
ä¼˜åŒ–å†…å®¹ï¼š
1. ç§»é™¤æ›´å¤šå†—ä½™ç‰¹å¾ï¼ˆnegative_ratio, bert_sentiment_meanï¼‰
2. æ”¹è¿›ç°‡å‘½åé€»è¾‘ï¼Œå¢å¼ºå·®å¼‚åŒ–
3. å¢å¼ºå•†ä¸šå»ºè®®ç”Ÿæˆ
4. ä¼˜åŒ–å¯è§†åŒ–æ•ˆæœ
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.font_manager import FontProperties
import warnings
import os
from datetime import datetime

# æœºå™¨å­¦ä¹ åº“
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import (
    silhouette_score, silhouette_samples,
    calinski_harabasz_score, davies_bouldin_score
)
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
from scipy import stats

warnings.filterwarnings('ignore')

# ============================================================================
# é…ç½®åŒº
# ============================================================================

# ä¸­æ–‡å­—ä½“é…ç½®
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

# èšç±»é¢œè‰²æ–¹æ¡ˆ
CLUSTER_COLORS = [
    '#E74C3C',  # çº¢
    '#3498DB',  # è“
    '#2ECC71',  # ç»¿
    '#9B59B6',  # ç´«
    '#F39C12',  # æ©™
    '#1ABC9C',  # é’
    '#E91E63',  # ç²‰
    '#34495E',  # æ·±ç°
    '#00BCD4',  # é’è“
    '#FF5722',  # æ·±æ©™
]

# å†—ä½™ç‰¹å¾åˆ—è¡¨ï¼ˆå®Œå…¨ç›¸å…³æˆ–é«˜åº¦ç›¸å…³ï¼‰
REDUNDANT_FEATURES = [
    'negative_ratio',       # ä¸ positive_ratio å®Œå…¨äº’è¡¥ (r=-1)
    'bert_sentiment_mean',  # ä¸ positive_ratio å®Œå…¨ç›¸å…³ (r=1)
]


class ClusteringPipelineOptimized:
    """
    å¨åˆ€å¸‚åœºèšç±»åˆ†ææµæ°´çº¿ - ä¼˜åŒ–ç‰ˆ

    Features:
    - è‡ªåŠ¨ç§»é™¤å†—ä½™ç‰¹å¾
    - K-Means++ èšç±»ï¼ˆç¨³å®šæ€§æœ€ä½³ï¼‰
    - æ™ºèƒ½ç°‡å‘½å
    - ä¸°å¯Œçš„å•†ä¸šæ´å¯Ÿ
    """

    def __init__(self, data_path: str = 'clustering_features_only.csv',
                 output_dir: str = 'clustering_results'):
        """
        åˆå§‹åŒ–èšç±»åˆ†ææµæ°´çº¿

        Args:
            data_path: ç‰¹å¾æ•°æ®æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
        """
        self.data_path = data_path
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        # åŠ è½½æ•°æ®
        self.df = pd.read_csv(data_path)

        # è¯†åˆ«IDåˆ—
        self.asin_col = 'asin' if 'asin' in self.df.columns else self.df.columns[0]

        # åˆ†ç¦»IDå’Œç‰¹å¾
        self.asins = self.df[self.asin_col].values
        self.feature_cols = [c for c in self.df.columns if c != self.asin_col]
        self.X_raw = self.df[self.feature_cols].values

        # åˆå§‹åŒ–å˜é‡
        self.X_scaled = None
        self.scaler = None
        self.pca = None
        self.X_pca = None
        self.X_tsne = None
        self.best_k = None
        self.best_labels = None
        self.best_algorithm = None
        self.cluster_profiles = None
        self.cluster_profiles_z = None
        self.cluster_descriptions = None
        self.cluster_names = None
        self.df_clustered = None
        self.metrics = None
        self.algorithm_results = {}

        # æ‰“å°åŠ è½½ä¿¡æ¯
        print("=" * 70)
        print("          å¨åˆ€å¸‚åœºèšç±»åˆ†æ (ä¼˜åŒ–ç‰ˆ v2.0)")
        print("=" * 70)
        print(f"\nğŸ“Š æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"   - æ ·æœ¬æ•°: {len(self.df)}")
        print(f"   - åŸå§‹ç‰¹å¾æ•°: {len(self.feature_cols)}")
        print(f"   - IDåˆ—: {self.asin_col}")

    def preprocess(self, scaler_type: str = 'standard',
                   remove_redundant: bool = True):
        """
        ç‰¹å¾é¢„å¤„ç†

        Args:
            scaler_type: æ ‡å‡†åŒ–æ–¹å¼ ('standard' æˆ– 'robust')
            remove_redundant: æ˜¯å¦ç§»é™¤å†—ä½™ç‰¹å¾
        """
        print("\n" + "=" * 70)
        print("[Step 1] ç‰¹å¾é¢„å¤„ç†")
        print("=" * 70)

        # ========== ç§»é™¤å†—ä½™ç‰¹å¾ ==========
        if remove_redundant:
            removed = []
            for feat in REDUNDANT_FEATURES:
                if feat in self.feature_cols:
                    removed.append(feat)
                    self.feature_cols.remove(feat)

            if removed:
                print(f"\n  ğŸ”§ ç§»é™¤å†—ä½™ç‰¹å¾: {removed}")
                self.X_raw = self.df[self.feature_cols].values

        print(f"  ğŸ“Š æœ‰æ•ˆç‰¹å¾æ•°: {len(self.feature_cols)}")

        # ========== å¤„ç†ç¼ºå¤±å€¼å’Œæ— ç©·å€¼ ==========
        X = self.X_raw.copy()

        # ç¼ºå¤±å€¼å¡«å……
        nan_counts = np.isnan(X).sum(axis=0)
        if nan_counts.sum() > 0:
            nan_features = [self.feature_cols[i] for i, c in enumerate(nan_counts) if c > 0]
            print(f"\n  âš ï¸ å‘ç°ç¼ºå¤±å€¼ç‰¹å¾: {nan_features[:5]}...")
            print(f"     ä½¿ç”¨ä¸­ä½æ•°å¡«å……...")
            for i, count in enumerate(nan_counts):
                if count > 0:
                    col_median = np.nanmedian(X[:, i])
                    X[np.isnan(X[:, i]), i] = col_median

        # æ— ç©·å€¼å¤„ç†
        inf_mask = np.isinf(X)
        if inf_mask.sum() > 0:
            print(f"  âš ï¸ å‘ç°æ— ç©·å€¼ï¼Œè½¬æ¢ä¸ºä¸­ä½æ•°...")
            X[inf_mask] = np.nan
            for i in range(X.shape[1]):
                col_median = np.nanmedian(X[:, i])
                X[np.isnan(X[:, i]), i] = col_median

        # ========== æ ‡å‡†åŒ– ==========
        print(f"\n  ğŸ“ ä½¿ç”¨ {scaler_type.title()}Scaler æ ‡å‡†åŒ–")

        if scaler_type == 'standard':
            self.scaler = StandardScaler()
        else:
            self.scaler = RobustScaler()

        self.X_scaled = self.scaler.fit_transform(X)

        # éªŒè¯æ ‡å‡†åŒ–ç»“æœ
        means = np.mean(self.X_scaled, axis=0)
        stds = np.std(self.X_scaled, axis=0)

        print(f"\n  âœ… æ ‡å‡†åŒ–å®Œæˆ:")
        print(f"     - å‡å€¼èŒƒå›´: [{means.min():.4f}, {means.max():.4f}]")
        print(f"     - æ ‡å‡†å·®èŒƒå›´: [{stds.min():.4f}, {stds.max():.4f}]")

        return self

    def analyze_features(self):
        """ç‰¹å¾ç›¸å…³æ€§åˆ†æ"""
        print("\n" + "-" * 50)
        print("ç‰¹å¾ç›¸å…³æ€§åˆ†æ")
        print("-" * 50)

        # è®¡ç®—ç›¸å…³çŸ©é˜µ
        corr_matrix = pd.DataFrame(self.X_scaled, columns=self.feature_cols).corr()

        # æ‰¾é«˜ç›¸å…³ç‰¹å¾å¯¹
        high_corr_pairs = []
        for i in range(len(self.feature_cols)):
            for j in range(i+1, len(self.feature_cols)):
                corr = corr_matrix.iloc[i, j]
                if abs(corr) > 0.8:
                    high_corr_pairs.append((self.feature_cols[i], self.feature_cols[j], corr))

        if high_corr_pairs:
            print("\n  âš ï¸ é«˜ç›¸å…³ç‰¹å¾å¯¹ (|r| > 0.8):")
            for f1, f2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:5]:
                print(f"     {f1} â†” {f2}: r = {corr:.3f}")
        else:
            print("\n  âœ… æ— é«˜åº¦ç›¸å…³çš„ç‰¹å¾å¯¹")

        # ç»˜åˆ¶ç›¸å…³æ€§çƒ­åŠ›å›¾
        plt.figure(figsize=(16, 14))
        mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
        sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='RdBu_r',
                    center=0, square=True, linewidths=0.5,
                    cbar_kws={'shrink': 0.8})
        plt.title('Feature Correlation Matrix', fontsize=14, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'correlation_matrix.png'), dpi=150, bbox_inches='tight')
        plt.close()

        print(f"\n  ğŸ“Š ç›¸å…³æ€§çƒ­åŠ›å›¾å·²ä¿å­˜: correlation_matrix.png")

        return self

    def reduce_dimensions(self, n_pca_components: int = 10):
        """
        é™ç»´åˆ†æï¼šPCA + t-SNE

        Args:
            n_pca_components: PCAä¿ç•™çš„ä¸»æˆåˆ†æ•°
        """
        print("\n" + "-" * 50)
        print("é™ç»´åˆ†æ")
        print("-" * 50)

        # ========== PCA ==========
        n_components = min(n_pca_components, self.X_scaled.shape[1], self.X_scaled.shape[0])
        self.pca = PCA(n_components=n_components)
        self.X_pca = self.pca.fit_transform(self.X_scaled)

        # ç´¯ç§¯æ–¹å·®è§£é‡Š
        cumsum_var = np.cumsum(self.pca.explained_variance_ratio_)

        print(f"\n  ğŸ“‰ PCA é™ç»´ç»“æœ:")
        print(f"     - ä¿ç•™ {n_components} ä¸ªä¸»æˆåˆ†")
        print(f"     - å‰2ä¸ªPCè§£é‡Šæ–¹å·®: {cumsum_var[1]*100:.1f}%")

        # æ‰¾åˆ°è¾¾åˆ°90%å’Œ95%æ–¹å·®æ‰€éœ€çš„PCæ•°
        n_90 = np.argmax(cumsum_var >= 0.90) + 1 if np.any(cumsum_var >= 0.90) else n_components
        n_95 = np.argmax(cumsum_var >= 0.95) + 1 if np.any(cumsum_var >= 0.95) else n_components
        print(f"     - è¾¾åˆ°90%æ–¹å·®éœ€: {n_90} ä¸ªPC")
        print(f"     - è¾¾åˆ°95%æ–¹å·®éœ€: {n_95} ä¸ªPC")

        # ç»˜åˆ¶æ–¹å·®è§£é‡Šå›¾
        fig, axes = plt.subplots(1, 2, figsize=(12, 4))

        # å•ç‹¬æ–¹å·®
        ax = axes[0]
        bars = ax.bar(range(1, n_components+1), self.pca.explained_variance_ratio_,
                     color='#3498db', alpha=0.8, edgecolor='white')
        ax.set_xlabel('Principal Component', fontsize=10)
        ax.set_ylabel('Explained Variance Ratio', fontsize=10)
        ax.set_title('PCA Explained Variance', fontsize=12, fontweight='bold')
        ax.set_xticks(range(1, n_components+1))

        # ç´¯ç§¯æ–¹å·®
        ax = axes[1]
        ax.plot(range(1, n_components+1), cumsum_var, 'o-', color='#e74c3c',
               linewidth=2, markersize=8)
        ax.axhline(y=0.9, color='gray', linestyle='--', alpha=0.7, label='90%')
        ax.axhline(y=0.95, color='gray', linestyle=':', alpha=0.7, label='95%')
        ax.fill_between(range(1, n_components+1), cumsum_var, alpha=0.2, color='#e74c3c')
        ax.set_xlabel('Number of Components', fontsize=10)
        ax.set_ylabel('Cumulative Explained Variance', fontsize=10)
        ax.set_title('Cumulative Variance Explained', fontsize=12, fontweight='bold')
        ax.set_xticks(range(1, n_components+1))
        ax.legend(loc='lower right')
        ax.set_ylim(0, 1.05)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'pca_variance.png'), dpi=150, bbox_inches='tight')
        plt.close()

        # ========== t-SNE ==========
        print("\n  ğŸ”„ æ‰§è¡Œ t-SNE é™ç»´...")
        perplexity = min(30, len(self.X_scaled) - 1)

        # å…¼å®¹æ–°æ—§ç‰ˆæœ¬ scikit-learn
        try:
            tsne = TSNE(n_components=2, perplexity=perplexity,
                        random_state=42, max_iter=1000)
        except TypeError:
            tsne = TSNE(n_components=2, perplexity=perplexity,
                        random_state=42, n_iter=1000)

        self.X_tsne = tsne.fit_transform(self.X_scaled)
        print("  âœ… t-SNE å®Œæˆ")

        return self

    def find_optimal_k(self, k_range: tuple = (3, 8)):
        """
        ç¡®å®šæœ€ä¼˜èšç±»æ•°

        Args:
            k_range: æœç´¢èŒƒå›´ (min_k, max_k)
        """
        print("\n" + "=" * 70)
        print("[Step 2] ç¡®å®šæœ€ä¼˜èšç±»æ•° (K-Means++)")
        print("=" * 70)

        k_min, k_max = k_range
        k_values = list(range(k_min, k_max + 1))

        metrics = {
            'k': k_values,
            'inertia': [],
            'silhouette': [],
            'calinski': [],
            'davies_bouldin': []
        }

        print(f"\n  ğŸ” æœç´¢èŒƒå›´: k = {k_min} ~ {k_max}")
        print("\n  " + "-" * 60)
        print(f"  {'K':<5} {'Inertia':<12} {'Silhouette':<12} {'CH Index':<12} {'DB Index':<10}")
        print("  " + "-" * 60)

        for k in k_values:
            kmeans = KMeans(n_clusters=k, init='k-means++', n_init=30,
                           max_iter=500, random_state=42)
            labels = kmeans.fit_predict(self.X_scaled)

            inertia = kmeans.inertia_
            silhouette = silhouette_score(self.X_scaled, labels)
            calinski = calinski_harabasz_score(self.X_scaled, labels)
            db_score = davies_bouldin_score(self.X_scaled, labels)

            metrics['inertia'].append(inertia)
            metrics['silhouette'].append(silhouette)
            metrics['calinski'].append(calinski)
            metrics['davies_bouldin'].append(db_score)

            print(f"  {k:<5} {inertia:<12.1f} {silhouette:<12.4f} {calinski:<12.1f} {db_score:<10.4f}")

        print("  " + "-" * 60)

        # ========== ç»¼åˆè¯„åˆ†é€‰æ‹©æœ€ä¼˜k ==========
        # æ ‡å‡†åŒ–å„æŒ‡æ ‡åˆ° [0, 1]
        sil_arr = np.array(metrics['silhouette'])
        ch_arr = np.array(metrics['calinski'])
        db_arr = np.array(metrics['davies_bouldin'])

        sil_norm = (sil_arr - sil_arr.min()) / (sil_arr.max() - sil_arr.min() + 1e-8)
        ch_norm = (ch_arr - ch_arr.min()) / (ch_arr.max() - ch_arr.min() + 1e-8)
        db_norm = 1 - (db_arr - db_arr.min()) / (db_arr.max() - db_arr.min() + 1e-8)  # DBè¶Šå°è¶Šå¥½

        # ç»¼åˆå¾—åˆ†ï¼ˆè½®å»“ç³»æ•°æƒé‡æœ€é«˜ï¼‰
        composite_score = 0.5 * sil_norm + 0.3 * ch_norm + 0.2 * db_norm
        best_idx = np.argmax(composite_score)
        self.best_k = k_values[best_idx]

        print(f"\n  ğŸ“Š å„æŒ‡æ ‡æ¨è:")
        print(f"     - è½®å»“ç³»æ•°æœ€ä¼˜: k = {k_values[np.argmax(sil_arr)]} (score = {sil_arr.max():.4f})")
        print(f"     - CH Indexæœ€ä¼˜: k = {k_values[np.argmax(ch_arr)]}")
        print(f"     - DB Indexæœ€ä¼˜: k = {k_values[np.argmin(db_arr)]}")
        print(f"\n  ğŸ¯ ç»¼åˆè¯„åˆ†æœ€ä¼˜: k = {self.best_k} (Silhouette = {metrics['silhouette'][best_idx]:.4f})")

        # ç»˜åˆ¶è¯„ä¼°å›¾
        self._plot_metrics(metrics, k_values, self.best_k)

        self.metrics = metrics
        return self

    def _plot_metrics(self, metrics, k_values, best_k):
        """ç»˜åˆ¶èšç±»æ•°è¯„ä¼°å›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))

        # 1. è‚˜éƒ¨æ³•åˆ™
        ax = axes[0, 0]
        ax.plot(k_values, metrics['inertia'], 'bo-', linewidth=2, markersize=8)
        ax.axvline(x=best_k, color='red', linestyle='--', linewidth=2, label=f'Best k={best_k}')
        ax.set_xlabel('Number of Clusters (K)', fontsize=10)
        ax.set_ylabel('Inertia (SSE)', fontsize=10)
        ax.set_title('Elbow Method', fontsize=12, fontweight='bold')
        ax.legend()
        ax.grid(True, alpha=0.3)
        ax.set_xticks(k_values)

        # 2. è½®å»“ç³»æ•°
        ax = axes[0, 1]
        colors = ['#e74c3c' if k == best_k else '#3498db' for k in k_values]
        bars = ax.bar(k_values, metrics['silhouette'], color=colors, edgecolor='white', linewidth=1.5)
        ax.set_xlabel('Number of Clusters (K)', fontsize=10)
        ax.set_ylabel('Silhouette Score', fontsize=10)
        ax.set_title('Silhouette Score', fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3, axis='y')
        ax.set_xticks(k_values)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for bar, val in zip(bars, metrics['silhouette']):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,
                   f'{val:.3f}', ha='center', va='bottom', fontsize=8)

        # 3. Calinski-Harabasz Index
        ax = axes[1, 0]
        ax.plot(k_values, metrics['calinski'], 'go-', linewidth=2, markersize=8)
        ax.axvline(x=best_k, color='red', linestyle='--', linewidth=2)
        ax.set_xlabel('Number of Clusters (K)', fontsize=10)
        ax.set_ylabel('Calinski-Harabasz Index', fontsize=10)
        ax.set_title('Calinski-Harabasz Index (Higher=Better)', fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.set_xticks(k_values)

        # 4. Davies-Bouldin Index
        ax = axes[1, 1]
        ax.plot(k_values, metrics['davies_bouldin'], 'mo-', linewidth=2, markersize=8)
        ax.axvline(x=best_k, color='red', linestyle='--', linewidth=2)
        ax.set_xlabel('Number of Clusters (K)', fontsize=10)
        ax.set_ylabel('Davies-Bouldin Index', fontsize=10)
        ax.set_title('Davies-Bouldin Index (Lower=Better)', fontsize=12, fontweight='bold')
        ax.grid(True, alpha=0.3)
        ax.set_xticks(k_values)

        plt.suptitle(f'Optimal K Analysis (Best: k={best_k})', fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'optimal_k_analysis.png'), dpi=150, bbox_inches='tight')
        plt.close()

        print(f"\n  ğŸ“Š èšç±»æ•°è¯„ä¼°å›¾å·²ä¿å­˜: optimal_k_analysis.png")

    def run_clustering(self, k: int = None):
        """
        æ‰§è¡Œ K-Means++ èšç±»

        Args:
            k: èšç±»æ•°ï¼Œé»˜è®¤ä½¿ç”¨è‡ªåŠ¨æ¨èçš„æœ€ä¼˜k
        """
        if k is None:
            k = self.best_k

        print("\n" + "=" * 70)
        print(f"[Step 3] æ‰§è¡Œ K-Means++ èšç±» (k = {k})")
        print("=" * 70)

        # K-Means++ èšç±»ï¼ˆå¢åŠ n_initä»¥æé«˜ç¨³å®šæ€§ï¼‰
        kmeans = KMeans(n_clusters=k, init='k-means++', n_init=50,
                        max_iter=500, random_state=42)
        self.best_labels = kmeans.fit_predict(self.X_scaled)
        self.best_algorithm = 'K-Means++'
        self.kmeans_model = kmeans

        # è®¡ç®—èšç±»è´¨é‡æŒ‡æ ‡
        sil = silhouette_score(self.X_scaled, self.best_labels)
        ch = calinski_harabasz_score(self.X_scaled, self.best_labels)
        db = davies_bouldin_score(self.X_scaled, self.best_labels)

        print(f"\n  ğŸ“Š èšç±»è´¨é‡æŒ‡æ ‡:")
        print(f"     - è½®å»“ç³»æ•° (Silhouette): {sil:.4f}")
        print(f"     - CH Index: {ch:.1f}")
        print(f"     - DB Index: {db:.4f}")

        # ========== ç°‡å¤§å°åˆ†å¸ƒ ==========
        cluster_sizes = pd.Series(self.best_labels).value_counts().sort_index()

        print(f"\n  ğŸ“Š ç°‡å¤§å°åˆ†å¸ƒ:")
        for cluster_id in sorted(cluster_sizes.index):
            size = cluster_sizes[cluster_id]
            pct = size / len(self.best_labels) * 100
            bar_len = int(pct / 3)
            bar = "â–ˆ" * bar_len
            print(f"     Cluster {cluster_id}: {size:>4} ({pct:>5.1f}%) {bar}")

        # ========== å¹³è¡¡æ€§æ£€æŸ¥ ==========
        min_size = cluster_sizes.min()
        max_size = cluster_sizes.max()
        balance_ratio = min_size / max_size

        print(f"\n  ğŸ“Š å¹³è¡¡æ€§æ£€æŸ¥:")
        print(f"     - æœ€å°ç°‡: {min_size} ({min_size/len(self.best_labels)*100:.1f}%)")
        print(f"     - æœ€å¤§ç°‡: {max_size} ({max_size/len(self.best_labels)*100:.1f}%)")
        print(f"     - å¹³è¡¡æ¯”: {balance_ratio:.3f}")

        if balance_ratio < 0.05:
            print(f"\n  âš ï¸ ä¸¥é‡è­¦å‘Šï¼šç°‡å¤§å°æåº¦ä¸å¹³è¡¡ï¼")
        elif balance_ratio < 0.1:
            print(f"\n  âš ï¸ è­¦å‘Šï¼šç°‡å¤§å°ä¸¥é‡ä¸å¹³è¡¡ï¼Œè€ƒè™‘è°ƒæ•´kå€¼")
        elif balance_ratio < 0.2:
            print(f"\n  âš ï¸ æ³¨æ„ï¼šç°‡å¤§å°å­˜åœ¨ä¸€å®šä¸å¹³è¡¡")
        else:
            print(f"\n  âœ… ç°‡å¤§å°åˆ†å¸ƒè¾ƒä¸ºå‡è¡¡")

        # å­˜å‚¨ç®—æ³•æ¯”è¾ƒç»“æœ
        self.algorithm_results = {
            'K-Means++': {
                'labels': self.best_labels,
                'silhouette': sil,
                'calinski': ch,
                'davies_bouldin': db
            }
        }

        # ç»˜åˆ¶èšç±»ç»“æœå¯è§†åŒ–
        self._plot_clustering_results()

        return self

    def _plot_clustering_results(self):
        """ç»˜åˆ¶èšç±»ç»“æœå¯è§†åŒ–"""
        n_clusters = len(np.unique(self.best_labels))

        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        # t-SNE å¯è§†åŒ–
        ax = axes[0]
        for i in range(n_clusters):
            mask = self.best_labels == i
            ax.scatter(self.X_tsne[mask, 0], self.X_tsne[mask, 1],
                      c=CLUSTER_COLORS[i % len(CLUSTER_COLORS)],
                      label=f'Cluster {i} (n={mask.sum()})',
                      alpha=0.6, s=50, edgecolors='white', linewidth=0.5)
        ax.set_xlabel('t-SNE Dimension 1', fontsize=10)
        ax.set_ylabel('t-SNE Dimension 2', fontsize=10)
        ax.set_title('Cluster Distribution (t-SNE)', fontsize=12, fontweight='bold')
        ax.legend(loc='best', fontsize=8)

        # PCA å¯è§†åŒ–
        ax = axes[1]
        for i in range(n_clusters):
            mask = self.best_labels == i
            ax.scatter(self.X_pca[mask, 0], self.X_pca[mask, 1],
                      c=CLUSTER_COLORS[i % len(CLUSTER_COLORS)],
                      label=f'Cluster {i}',
                      alpha=0.6, s=50, edgecolors='white', linewidth=0.5)
        ax.set_xlabel(f'PC1 ({self.pca.explained_variance_ratio_[0]*100:.1f}%)', fontsize=10)
        ax.set_ylabel(f'PC2 ({self.pca.explained_variance_ratio_[1]*100:.1f}%)', fontsize=10)
        ax.set_title('Cluster Distribution (PCA)', fontsize=12, fontweight='bold')

        plt.suptitle(f'K-Means++ Clustering Results (k={n_clusters})', fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'clustering_results.png'), dpi=150, bbox_inches='tight')
        plt.close()

        print(f"\n  ğŸ“Š èšç±»ç»“æœå›¾å·²ä¿å­˜: clustering_results.png")

    def analyze_clusters(self):
        """èšç±»ç»“æœæ·±å…¥åˆ†æ"""
        print("\n" + "=" * 70)
        print("[Step 4] èšç±»ç»“æœåˆ†æ")
        print("=" * 70)

        # åˆ›å»ºåˆ†ææ•°æ®æ¡†
        df_analysis = self.df[self.feature_cols].copy()
        df_analysis['cluster'] = self.best_labels
        df_analysis[self.asin_col] = self.asins

        n_clusters = len(np.unique(self.best_labels))

        # ========== è®¡ç®—å„ç°‡ç‰¹å¾å‡å€¼ ==========
        cluster_profiles = df_analysis.groupby('cluster')[self.feature_cols].mean()

        # ========== è®¡ç®— Z-score æ ‡å‡†åŒ–çš„ç‰¹å¾ç”»åƒ ==========
        overall_mean = df_analysis[self.feature_cols].mean()
        overall_std = df_analysis[self.feature_cols].std()
        cluster_profiles_z = (cluster_profiles - overall_mean) / (overall_std + 1e-8)

        self.cluster_profiles = cluster_profiles
        self.cluster_profiles_z = cluster_profiles_z

        # ========== å„ç°‡æ˜¾è‘—ç‰¹å¾åˆ†æ ==========
        print("\n  ğŸ” å„ç°‡æ˜¾è‘—ç‰¹å¾ (z-score > 0.5 æˆ– < -0.5):")
        print("  " + "-" * 60)

        cluster_descriptions = {}
        cluster_sizes = df_analysis['cluster'].value_counts().sort_index()

        for cluster_id in range(n_clusters):
            z_scores = cluster_profiles_z.loc[cluster_id]

            # é«˜äºå¹³å‡çš„ç‰¹å¾
            high_features = z_scores[z_scores > 0.5].sort_values(ascending=False)
            # ä½äºå¹³å‡çš„ç‰¹å¾
            low_features = z_scores[z_scores < -0.5].sort_values()

            size = cluster_sizes[cluster_id]
            pct = size / len(df_analysis) * 100

            print(f"\n  ã€Cluster {cluster_id}ã€‘ ({size} ä¸ªå•†å“, {pct:.1f}%)")

            if len(high_features) > 0:
                print(f"    â†‘ é«˜äºå¹³å‡:")
                for feat, val in high_features.head(5).items():
                    print(f"       {feat}: z = {val:+.2f}")

            if len(low_features) > 0:
                print(f"    â†“ ä½äºå¹³å‡:")
                for feat, val in low_features.head(5).items():
                    print(f"       {feat}: z = {val:+.2f}")

            if len(high_features) == 0 and len(low_features) == 0:
                print(f"    (ç‰¹å¾æ¥è¿‘å¸‚åœºå¹³å‡)")

            cluster_descriptions[cluster_id] = {
                'size': size,
                'pct': pct,
                'high_features': high_features.head(5).to_dict(),
                'low_features': low_features.head(5).to_dict()
            }

        self.cluster_descriptions = cluster_descriptions
        self.df_clustered = df_analysis

        # ç”Ÿæˆå¯è§†åŒ–
        self._plot_cluster_analysis_dashboard(n_clusters)
        self._plot_radar_chart(n_clusters)
        self._plot_dendrogram()

        return self

    def _plot_cluster_analysis_dashboard(self, n_clusters):
        """ç”Ÿæˆèšç±»åˆ†æç»¼åˆé¢æ¿"""
        fig = plt.figure(figsize=(18, 14))

        # 1. t-SNE å¯è§†åŒ–
        ax1 = fig.add_subplot(2, 3, 1)
        for i in range(n_clusters):
            mask = self.best_labels == i
            ax1.scatter(self.X_tsne[mask, 0], self.X_tsne[mask, 1],
                       c=CLUSTER_COLORS[i % len(CLUSTER_COLORS)],
                       label=f'C{i} (n={mask.sum()})', alpha=0.6, s=40)
        ax1.set_title('t-SNE Visualization', fontsize=11, fontweight='bold')
        ax1.legend(fontsize=7, loc='best')
        ax1.set_xlabel('t-SNE 1')
        ax1.set_ylabel('t-SNE 2')

        # 2. ç°‡å¤§å°åˆ†å¸ƒé¥¼å›¾
        ax2 = fig.add_subplot(2, 3, 2)
        sizes = pd.Series(self.best_labels).value_counts().sort_index()
        colors_pie = [CLUSTER_COLORS[i % len(CLUSTER_COLORS)] for i in range(n_clusters)]
        wedges, texts, autotexts = ax2.pie(
            sizes,
            labels=[f'C{i}' for i in range(n_clusters)],
            autopct=lambda pct: f'{pct:.1f}%\n({int(pct/100*len(self.best_labels))})',
            colors=colors_pie,
            explode=[0.02]*n_clusters,
            textprops={'fontsize': 9}
        )
        ax2.set_title('Cluster Size Distribution', fontsize=11, fontweight='bold')

        # 3. è½®å»“ç³»æ•°åˆ†æå›¾
        ax3 = fig.add_subplot(2, 3, 3)
        silhouette_vals = silhouette_samples(self.X_scaled, self.best_labels)
        y_lower = 10

        for i in range(n_clusters):
            cluster_silhouette = silhouette_vals[self.best_labels == i]
            cluster_silhouette.sort()
            size_cluster = cluster_silhouette.shape[0]
            y_upper = y_lower + size_cluster

            ax3.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_silhouette,
                             facecolor=CLUSTER_COLORS[i % len(CLUSTER_COLORS)],
                             alpha=0.7, edgecolor='white')
            ax3.text(-0.05, y_lower + 0.5 * size_cluster, str(i), fontsize=9)
            y_lower = y_upper + 10

        avg_sil = silhouette_vals.mean()
        ax3.axvline(x=avg_sil, color='red', linestyle='--', linewidth=2,
                   label=f'Avg: {avg_sil:.3f}')
        ax3.set_xlabel('Silhouette Coefficient', fontsize=10)
        ax3.set_ylabel('Cluster', fontsize=10)
        ax3.set_title('Silhouette Analysis', fontsize=11, fontweight='bold')
        ax3.legend(loc='upper right')

        # 4. å…³é”®ç‰¹å¾ç®±çº¿å›¾
        ax4 = fig.add_subplot(2, 3, 4)
        key_feats = ['log_price', 'product_rating', 'log_sales', 'weighted_rating']
        key_feats = [f for f in key_feats if f in self.feature_cols][:3]

        if key_feats:
            plot_data = self.df_clustered[['cluster'] + key_feats].melt(
                id_vars='cluster', var_name='Feature', value_name='Value')
            sns.boxplot(data=plot_data, x='Feature', y='Value', hue='cluster',
                       palette=CLUSTER_COLORS[:n_clusters], ax=ax4)
            ax4.set_title('Key Features by Cluster', fontsize=11, fontweight='bold')
            ax4.legend(title='Cluster', fontsize=7, title_fontsize=8)
            ax4.tick_params(axis='x', rotation=15)

        # 5. ç‰¹å¾çƒ­åŠ›å›¾ï¼ˆTop 15 ç‰¹å¾ï¼‰
        ax5 = fig.add_subplot(2, 3, 5)
        top_features = self.cluster_profiles_z.abs().mean().nlargest(15).index.tolist()
        heatmap_data = self.cluster_profiles_z[top_features].T

        sns.heatmap(heatmap_data, annot=True, fmt='.2f', cmap='RdBu_r',
                   center=0, ax=ax5, cbar_kws={'shrink': 0.8},
                   xticklabels=[f'C{i}' for i in range(n_clusters)],
                   annot_kws={'size': 8})
        ax5.set_title('Feature Z-Scores Heatmap (Top 15)', fontsize=11, fontweight='bold')
        ax5.tick_params(axis='y', rotation=0, labelsize=8)

        # 6. ç°‡é—´è·ç¦»çŸ©é˜µ
        ax6 = fig.add_subplot(2, 3, 6)
        cluster_centers = self.cluster_profiles.values
        from scipy.spatial.distance import pdist, squareform
        dist_matrix = squareform(pdist(cluster_centers, metric='euclidean'))

        sns.heatmap(dist_matrix, annot=True, fmt='.2f', cmap='YlOrRd',
                   xticklabels=[f'C{i}' for i in range(n_clusters)],
                   yticklabels=[f'C{i}' for i in range(n_clusters)],
                   ax=ax6, cbar_kws={'shrink': 0.8})
        ax6.set_title('Inter-Cluster Distance', fontsize=11, fontweight='bold')

        plt.suptitle('Cluster Analysis Dashboard', fontsize=14, fontweight='bold', y=1.02)
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'cluster_analysis_dashboard.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()

        print(f"\n  ğŸ“Š èšç±»åˆ†æé¢æ¿å·²ä¿å­˜: cluster_analysis_dashboard.png")

    def _plot_radar_chart(self, n_clusters):
        """ç»˜åˆ¶é›·è¾¾å›¾"""
        # é€‰æ‹©å…³é”®ç‰¹å¾
        radar_feats = [
            'log_price', 'product_rating', 'log_sales', 'log_reviews',
            'is_set', 'positive_ratio', 'is_fba', 'discount_rate'
        ]
        radar_feats = [f for f in radar_feats if f in self.feature_cols][:8]

        if len(radar_feats) < 4:
            # å¦‚æœå…³é”®ç‰¹å¾ä¸è¶³ï¼Œé€‰æ‹©å˜å¼‚æœ€å¤§çš„ç‰¹å¾
            var_rank = self.cluster_profiles_z.var().nlargest(8).index.tolist()
            radar_feats = var_rank

        # å½’ä¸€åŒ–æ•°æ®
        radar_data = self.cluster_profiles[radar_feats].copy()
        radar_norm = (radar_data - radar_data.min()) / (radar_data.max() - radar_data.min() + 1e-8)

        # åˆ›å»ºé›·è¾¾å›¾
        angles = np.linspace(0, 2*np.pi, len(radar_feats), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆ

        fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(polar=True))

        for i in range(n_clusters):
            values = radar_norm.loc[i].tolist()
            values += values[:1]  # é—­åˆ

            ax.plot(angles, values, 'o-', linewidth=2, label=f'Cluster {i}',
                   color=CLUSTER_COLORS[i % len(CLUSTER_COLORS)], markersize=6)
            ax.fill(angles, values, alpha=0.15, color=CLUSTER_COLORS[i % len(CLUSTER_COLORS)])

        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(radar_feats, fontsize=10)
        ax.set_title('Cluster Profiles Radar Chart', fontsize=12, fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=9)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'cluster_radar_chart.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()

        print(f"  ğŸ“Š é›·è¾¾å›¾å·²ä¿å­˜: cluster_radar_chart.png")

    def _plot_dendrogram(self):
        """ç»˜åˆ¶å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾"""
        # é‡‡æ ·ï¼ˆå¤§æ•°æ®é›†æ—¶ï¼‰
        n_sample = min(200, len(self.X_scaled))
        np.random.seed(42)
        sample_idx = np.random.choice(len(self.X_scaled), n_sample, replace=False)
        X_sample = self.X_scaled[sample_idx]

        # è®¡ç®—å±‚æ¬¡èšç±»
        linkage_matrix = linkage(X_sample, method='ward')

        fig, ax = plt.subplots(figsize=(14, 6))
        dendrogram(linkage_matrix, ax=ax, truncate_mode='level', p=5,
                   leaf_rotation=90, leaf_font_size=8,
                   color_threshold=0.7*max(linkage_matrix[:,2]))

        ax.set_title('Hierarchical Clustering Dendrogram (Ward Method)',
                    fontsize=12, fontweight='bold')
        ax.set_xlabel('Sample Index', fontsize=10)
        ax.set_ylabel('Distance', fontsize=10)

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'dendrogram.png'), dpi=150, bbox_inches='tight')
        plt.close()

        print(f"  ğŸ“Š æ ‘çŠ¶å›¾å·²ä¿å­˜: dendrogram.png")

    def _auto_name_clusters(self):
        """æ™ºèƒ½å‘½åç°‡ - ä¿®æ­£ç‰ˆ"""
        cluster_names = {}
        n_clusters = len(np.unique(self.best_labels))

        for cluster_id in range(n_clusters):
            profile = self.cluster_profiles.loc[cluster_id]
            z = self.cluster_profiles_z.loc[cluster_id]
            desc = self.cluster_descriptions[cluster_id]
            size_pct = desc['pct']

            parts = []

            # ========== è§„æ¨¡æ ‡ç­¾ ==========
            if size_pct > 35:
                parts.append('ä¸»æµ')
            elif size_pct > 15:
                parts.append('ä¸­ç­‰')
            elif size_pct < 5:
                parts.append('ç»†åˆ†')

            # ========== ä»·æ ¼å®šä½ï¼ˆä¿®æ­£ç‰ˆï¼‰==========
            # æ³¨æ„ï¼šä½¿ç”¨ log_price_per_pieceï¼ˆå•ä»·ï¼‰è€Œé log_priceï¼ˆæ€»ä»·ï¼‰
            price_z = z.get('log_price_per_piece', z.get('log_price', 0))

            if price_z > 0.5:
                parts.append('é«˜å•ä»·')  # ä¿®æ­£ï¼šæ­£zå€¼=é«˜äºå¹³å‡
            elif price_z > 0.2:
                parts.append('ä¸­é«˜å•ä»·')
            elif price_z < -0.5:
                parts.append('ä½å•ä»·')  # ä¿®æ­£ï¼šè´Ÿzå€¼=ä½äºå¹³å‡
            elif price_z < -0.2:
                parts.append('å¹³ä»·')

            # ========== äº§å“å½¢æ€ ==========
            is_set_val = profile.get('is_set', 0.5)
            if is_set_val > 0.6:
                parts.append('å¥—è£…')
            elif is_set_val < 0.4:
                parts.append('å•å“')

            # ========== å£ç¢‘ç‰¹å¾ ==========
            sentiment_z = z.get('positive_ratio', z.get('aspect_sentiment_mean', 0))
            rating_z = z.get('product_rating', 0)

            if sentiment_z > 1.5 or rating_z > 1.0:
                parts.append('å£ç¢‘ä½³')
            elif sentiment_z < -2 or rating_z < -1.5:
                parts.append('å¾…æ”¹è¿›')

            # ========== é”€é‡/æ›å…‰ç‰¹å¾ ==========
            sales_z = z.get('log_sales', 0)
            reviews_z = z.get('log_reviews', 0)
            bsr_z = z.get('log_bsr', 0)

            if sales_z > 0.5:
                parts.append('ç•…é”€')
            elif sales_z < -1.0 or reviews_z < -1.5 or bsr_z < -1.5:
                parts.append('ä½æ›å…‰')

            # ========== æè´¨/é£æ ¼ç‰¹è‰² ==========
            if profile.get('is_damascus', 0) > 0.15:
                parts.append('å¤§é©¬å£«é©')
            elif profile.get('is_german_steel', 0) > 0.12:
                parts.append('å¾·ç³»')
            elif profile.get('is_japanese_steel', 0) > 0.12:
                parts.append('æ—¥ç³»')
            elif profile.get('is_ceramic', 0) > 0.08:
                parts.append('é™¶ç“·')

            # ç»„åˆåç§°ï¼ˆæœ€å¤šå–3ä¸ªæ ‡ç­¾ï¼‰
            if parts:
                cluster_names[cluster_id] = '-'.join(parts[:3])
            else:
                cluster_names[cluster_id] = f'å¸‚åœºç»†åˆ†{cluster_id}'

            # ========== ç‰¹æ®Šæƒ…å†µå¤„ç† ==========
            # å¦‚æœåªæœ‰"å¾…æ”¹è¿›"æ²¡æœ‰å…¶ä»–æ ‡ç­¾ï¼Œæ·»åŠ æ›´å¤šä¸Šä¸‹æ–‡
            if cluster_names[cluster_id] == 'å¾…æ”¹è¿›':
                if size_pct < 15:
                    cluster_names[cluster_id] = 'é•¿å°¾-å¾…æ¿€æ´»'
                else:
                    cluster_names[cluster_id] = 'è´¨é‡å¾…æ”¹è¿›'

        return cluster_names

    def _generate_suggestions(self, cluster_id, desc, z_scores):
        """
        ç”Ÿæˆå•†ä¸šå»ºè®® - åŸºäºç°‡ç‰¹å¾
        """
        suggestions = []
        high = desc['high_features']
        low = desc['low_features']

        # é«˜ç«¯å¸‚åœºå»ºè®®
        if z_scores.get('log_price', 0) > 0.5:
            suggestions.append("é«˜ç«¯å¸‚åœºå®šä½ï¼Œå¼ºåŒ–å“è´¨æ•…äº‹å’Œå“ç‰Œæº¢ä»·èƒ½åŠ›")

        # ç»æµå‹å¸‚åœºå»ºè®®
        if z_scores.get('log_price', 0) < -0.5:
            suggestions.append("ä»·æ ¼æ•æ„Ÿç¾¤ä½“ï¼Œå¯é€šè¿‡å¥—è£…ç»„åˆæˆ–å¢å€¼æœåŠ¡æå‡å®¢å•ä»·")

        # å£ç¢‘é—®é¢˜
        if 'positive_ratio' in low or 'aspect_sentiment_mean' in low:
            suggestions.append("å…³æ³¨ç”¨æˆ·è´Ÿé¢åé¦ˆï¼Œæ”¹å–„äº§å“è´¨é‡å’Œä½¿ç”¨ä½“éªŒ")

        # å£ç¢‘ä¼˜åŠ¿
        if 'positive_ratio' in high or 'aspect_sentiment_mean' in high:
            suggestions.append("å£ç¢‘æ˜¯æ ¸å¿ƒä¼˜åŠ¿ï¼Œé¼“åŠ±ç”¨æˆ·è¯„ä»·ï¼Œåšå¥½å£ç¢‘è¥é”€")

        # ç•…é”€å“
        if 'log_sales' in high:
            suggestions.append("çƒ­é”€å“ç±»ï¼Œå¯æµ‹è¯•ä»·æ ¼å¼¹æ€§ï¼Œé€‚å½“æä»·")

        # å°ä¼—å“
        if 'log_sales' in low or 'log_reviews' in low:
            suggestions.append("æ›å…‰ä¸è¶³ï¼ŒåŠ å¼ºå¹¿å‘ŠæŠ•æ”¾å’Œå…³é”®è¯ä¼˜åŒ–")

        # é˜²é”ˆç—›ç‚¹
        if 'aspect_rust_sentiment' in low:
            suggestions.append("é˜²é”ˆæ˜¯ç”¨æˆ·ç—›ç‚¹ï¼Œå¼ºè°ƒä¸é”ˆé’¢æè´¨æˆ–æä¾›ä¿å…»æŒ‡å—")

        # é”‹åˆ©ä¼˜åŠ¿
        if 'aspect_sharpness_sentiment' in high:
            suggestions.append("é”‹åˆ©åº¦æ˜¯äº§å“ä¼˜åŠ¿ï¼Œå¯ä½œä¸ºæ ¸å¿ƒå–ç‚¹é‡ç‚¹å®£ä¼ ")

        # å¥—è£…äº§å“
        if z_scores.get('is_set', 0) > 0.5:
            suggestions.append("å¥—è£…å¸‚åœºï¼Œå…³æ³¨åˆ€åº§è®¾è®¡å’Œç»„åˆæ­é…")

        # å•å“å¸‚åœº
        if z_scores.get('is_set', 0) < -0.3:
            suggestions.append("å•å“å¸‚åœºï¼Œä¸“æ³¨ä¸“ä¸šç”¨æˆ·éœ€æ±‚ï¼Œçªå‡ºä¸“ä¸šæ€§èƒ½")

        # é»˜è®¤å»ºè®®
        if not suggestions:
            suggestions.append("ç»´æŒç°æœ‰äº§å“ç­–ç•¥ï¼ŒæŒç»­ç›‘æµ‹å¸‚åœºç«äº‰åŠ¨æ€")

        return suggestions[:4]  # æœ€å¤šè¿”å›4æ¡å»ºè®®

    def generate_business_insights(self):
        """ç”Ÿæˆå•†ä¸šæ´å¯ŸæŠ¥å‘Š"""
        print("\n" + "=" * 70)
        print("[Step 5] å•†ä¸šæ´å¯ŸæŠ¥å‘Š")
        print("=" * 70)

        # è‡ªåŠ¨å‘½å
        cluster_names = self._auto_name_clusters()
        self.cluster_names = cluster_names

        print("\n  ğŸ·ï¸ æ™ºèƒ½ç°‡å‘½å:")
        for cid, name in cluster_names.items():
            size = self.cluster_descriptions[cid]['size']
            pct = self.cluster_descriptions[cid]['pct']
            print(f"     Cluster {cid}: {name} ({size}ä¸ª, {pct:.1f}%)")

        # ========== æ„å»ºæŠ¥å‘Š ==========
        report_lines = []
        report_lines.append("=" * 80)
        report_lines.append("          å¨åˆ€å¸‚åœºèšç±»åˆ†æ - å•†ä¸šæ´å¯ŸæŠ¥å‘Š")
        report_lines.append("=" * 80)
        report_lines.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append(f"åˆ†ææ ·æœ¬: {len(self.df)} ä¸ªå•†å“")
        report_lines.append(f"æœ‰æ•ˆç‰¹å¾: {len(self.feature_cols)} ä¸ª")
        report_lines.append(f"èšç±»æ•°é‡: {len(np.unique(self.best_labels))} ä¸ªç»†åˆ†å¸‚åœº")
        report_lines.append(f"èšç±»ç®—æ³•: {self.best_algorithm}")
        report_lines.append(f"è½®å»“ç³»æ•°: {silhouette_score(self.X_scaled, self.best_labels):.4f}")

        report_lines.append("\n" + "=" * 80)
        report_lines.append("                      å¸‚åœºç»†åˆ†æ¦‚è§ˆ")
        report_lines.append("=" * 80)

        n_clusters = len(np.unique(self.best_labels))

        for cluster_id in range(n_clusters):
            name = cluster_names[cluster_id]
            desc = self.cluster_descriptions[cluster_id]
            z_scores = self.cluster_profiles_z.loc[cluster_id]

            size = desc['size']
            pct = desc['pct']

            report_lines.append(f"\n{'â”€' * 80}")
            report_lines.append(f"ã€Cluster {cluster_id}ã€‘ {name}")
            report_lines.append(f"{'â”€' * 80}")
            report_lines.append(f"  ğŸ“Š è§„æ¨¡: {size} ä¸ªå•†å“ ({pct:.1f}%)")

            # æ ¸å¿ƒç‰¹å¾
            report_lines.append(f"\n  âœ… æ ¸å¿ƒä¼˜åŠ¿ (é«˜äºå¸‚åœºå¹³å‡):")
            if desc['high_features']:
                for feat, z in list(desc['high_features'].items())[:5]:
                    report_lines.append(f"      â€¢ {feat}: z = {z:+.2f}")
            else:
                report_lines.append(f"      (æ— æ˜¾è‘—é«˜äºå¹³å‡çš„ç‰¹å¾)")

            # å¼±åŠ¿ç‰¹å¾
            report_lines.append(f"\n  âš ï¸ æ”¹è¿›ç©ºé—´ (ä½äºå¸‚åœºå¹³å‡):")
            if desc['low_features']:
                for feat, z in list(desc['low_features'].items())[:5]:
                    report_lines.append(f"      â€¢ {feat}: z = {z:+.2f}")
            else:
                report_lines.append(f"      (æ— æ˜¾è‘—ä½äºå¹³å‡çš„ç‰¹å¾)")

            # å•†ä¸šå»ºè®®
            report_lines.append(f"\n  ğŸ’¡ å•†ä¸šå»ºè®®:")
            suggestions = self._generate_suggestions(cluster_id, desc, z_scores.to_dict())
            for i, sug in enumerate(suggestions, 1):
                report_lines.append(f"      {i}. {sug}")

        # ========== æ•´ä½“å¸‚åœºæ´å¯Ÿ ==========
        report_lines.append("\n" + "=" * 80)
        report_lines.append("                      æ•´ä½“å¸‚åœºæ´å¯Ÿ")
        report_lines.append("=" * 80)

        insights = self._generate_overall_insights()
        for insight in insights:
            report_lines.append(f"\n  {insight}")

        # ========== è¡ŒåŠ¨å»ºè®®æ±‡æ€» ==========
        report_lines.append("\n" + "=" * 80)
        report_lines.append("                      æˆ˜ç•¥å»ºè®®")
        report_lines.append("=" * 80)

        strategic_suggestions = self._generate_strategic_suggestions()
        for i, sug in enumerate(strategic_suggestions, 1):
            report_lines.append(f"\n  {i}. {sug}")

        report_lines.append("\n" + "=" * 80)
        report_lines.append("                        æŠ¥å‘Šç»“æŸ")
        report_lines.append("=" * 80)

        report_text = "\n".join(report_lines)

        # ä¿å­˜æŠ¥å‘Š
        report_path = os.path.join(self.output_dir, 'business_insights_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_text)

        # æ‰“å°æŠ¥å‘Š
        print(report_text)

        print(f"\n  ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {report_path}")

        return self

    def _generate_overall_insights(self):
        """ç”Ÿæˆæ•´ä½“å¸‚åœºæ´å¯Ÿ"""
        insights = []

        # å¸‚åœºé›†ä¸­åº¦
        cluster_sizes = pd.Series(self.best_labels).value_counts()
        top2_share = cluster_sizes.nlargest(2).sum() / len(self.best_labels)

        if top2_share > 0.7:
            insights.append(f"ğŸ“ˆ å¸‚åœºé«˜åº¦é›†ä¸­ï¼šå‰2ä¸ªç»†åˆ†å¸‚åœºå æ® {top2_share*100:.1f}% çš„ä»½é¢")
        elif top2_share > 0.5:
            insights.append(f"ğŸ“Š å¸‚åœºä¸­ç­‰é›†ä¸­ï¼šå‰2ä¸ªç»†åˆ†å¸‚åœºå  {top2_share*100:.1f}% çš„ä»½é¢")
        else:
            insights.append(f"ğŸ“Š å¸‚åœºè¾ƒä¸ºåˆ†æ•£ï¼šå‰2ä¸ªç»†åˆ†å¸‚åœºä»…å  {top2_share*100:.1f}%")

        # ä»·æ ¼åˆ†æ
        if 'log_price' in self.feature_cols:
            price_range = self.df_clustered['log_price'].max() - self.df_clustered['log_price'].min()
            insights.append(f"ğŸ’° ä»·æ ¼è·¨åº¦ï¼šå¯¹æ•°ä»·æ ¼èŒƒå›´ {price_range:.2f}ï¼Œå­˜åœ¨æ˜æ˜¾ä»·æ ¼åˆ†å±‚")

        # æƒ…æ„Ÿåˆ†æ
        if 'positive_ratio' in self.feature_cols:
            avg_sent = self.df_clustered['positive_ratio'].mean()
            insights.append(f"ğŸ’¬ æ•´ä½“ç”¨æˆ·æƒ…æ„Ÿï¼šå¹³å‡æ­£å‘æ¯”ä¾‹ {avg_sent:.3f}ï¼Œ{'æ•´ä½“æ­£é¢' if avg_sent > 0.5 else 'æœ‰æå‡ç©ºé—´'}")

        # å¥—è£…åˆ†æ
        if 'is_set' in self.feature_cols:
            set_ratio = self.df_clustered['is_set'].mean()
            insights.append(f"ğŸ“¦ äº§å“å½¢æ€ï¼š{set_ratio*100:.1f}% ä¸ºå¥—è£…å•†å“ï¼Œ{'å¥—è£…ä¸ºä¸»' if set_ratio > 0.5 else 'å•å“ä¸ºä¸»'}")

        # FBAåˆ†æ
        if 'is_fba' in self.feature_cols:
            fba_ratio = self.df_clustered['is_fba'].mean()
            insights.append(f"ğŸšš ç‰©æµæ¨¡å¼ï¼š{fba_ratio*100:.1f}% ä½¿ç”¨FBAé…é€")

        # è¯„åˆ†åˆ†å¸ƒ
        if 'product_rating' in self.feature_cols:
            avg_rating = self.df_clustered['product_rating'].mean()
            insights.append(f"â­ å¹³å‡è¯„åˆ†ï¼š{avg_rating:.2f} åˆ†")

        return insights

    def _generate_strategic_suggestions(self):
        """ç”Ÿæˆæˆ˜ç•¥å»ºè®®"""
        suggestions = []

        # åŸºäºå¸‚åœºç»“æ„çš„å»ºè®®
        cluster_sizes = pd.Series(self.best_labels).value_counts()
        largest_cluster = cluster_sizes.idxmax()
        smallest_cluster = cluster_sizes.idxmin()

        largest_name = self.cluster_names.get(largest_cluster, f'Cluster {largest_cluster}')
        smallest_name = self.cluster_names.get(smallest_cluster, f'Cluster {smallest_cluster}')

        suggestions.append(f"ã€ä¸»æˆ˜åœºã€‘'{largest_name}' æ˜¯æœ€å¤§ç»†åˆ†å¸‚åœºï¼Œç«äº‰æ¿€çƒˆï¼Œéœ€å·®å¼‚åŒ–å®šä½")
        suggestions.append(f"ã€è“æµ·æœºä¼šã€‘'{smallest_name}' æ˜¯å°ä¼—å¸‚åœºï¼Œå¯è¯„ä¼°æ˜¯å¦å­˜åœ¨æœªè¢«æ»¡è¶³çš„éœ€æ±‚")

        # åŸºäºå£ç¢‘çš„å»ºè®®
        if 'positive_ratio' in self.feature_cols:
            low_sentiment_clusters = []
            for cid in range(len(cluster_sizes)):
                z = self.cluster_profiles_z.loc[cid].get('positive_ratio', 0)
                if z < -1.5:
                    low_sentiment_clusters.append(cid)

            if low_sentiment_clusters:
                names = [self.cluster_names.get(c, f'C{c}') for c in low_sentiment_clusters]
                suggestions.append(f"ã€è´¨é‡è­¦ç¤ºã€‘{', '.join(names)} ç”¨æˆ·åé¦ˆè¾ƒå·®ï¼ŒäºŸéœ€äº§å“è´¨é‡æ”¹è¿›")

        # åŸºäºä»·æ ¼çš„å»ºè®®
        if 'log_price' in self.feature_cols:
            price_variance = self.cluster_profiles_z['log_price'].std()
            if price_variance > 0.8:
                suggestions.append("ã€ä»·æ ¼ç­–ç•¥ã€‘å„ç»†åˆ†å¸‚åœºä»·æ ¼å·®å¼‚æ˜æ˜¾ï¼Œå¯é’ˆå¯¹æ€§åˆ¶å®šå®šä»·ç­–ç•¥")
            else:
                suggestions.append("ã€ä»·æ ¼æœºä¼šã€‘å„ç»†åˆ†å¸‚åœºä»·æ ¼è¶‹åŒï¼Œå­˜åœ¨é«˜ç«¯åŒ–æˆ–æ€§ä»·æ¯”å·®å¼‚åŒ–ç©ºé—´")

        # å¥—è£…vså•å“
        if 'is_set' in self.feature_cols:
            set_clusters = []
            single_clusters = []
            for cid in range(len(cluster_sizes)):
                is_set_val = self.cluster_profiles.loc[cid].get('is_set', 0.5)
                if is_set_val > 0.7:
                    set_clusters.append(cid)
                elif is_set_val < 0.3:
                    single_clusters.append(cid)

            if set_clusters and single_clusters:
                suggestions.append("ã€äº§å“å½¢æ€ã€‘å¥—è£…ä¸å•å“å½¢æˆæ˜æ˜¾åˆ†é‡ï¼Œå¯æ ¹æ®ç›®æ ‡å¸‚åœºé€‰æ‹©äº§å“å½¢æ€")

        return suggestions

    def save_results(self):
        """ä¿å­˜æ‰€æœ‰ç»“æœ"""
        print("\n" + "=" * 70)
        print("[Step 6] ä¿å­˜ç»“æœ")
        print("=" * 70)

        # 1. å¸¦èšç±»æ ‡ç­¾çš„å®Œæ•´æ•°æ®
        output_df = self.df.copy()
        output_df['cluster'] = self.best_labels
        output_df['cluster_name'] = output_df['cluster'].map(self.cluster_names)
        output_df['tsne_x'] = self.X_tsne[:, 0]
        output_df['tsne_y'] = self.X_tsne[:, 1]
        output_df['pca_x'] = self.X_pca[:, 0]
        output_df['pca_y'] = self.X_pca[:, 1]

        output_path = os.path.join(self.output_dir, 'clustered_products.csv')
        output_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        print(f"  âœ… {output_path}")

        # 2. ç°‡ç‰¹å¾ç”»åƒï¼ˆåŸå§‹å‡å€¼ï¼‰
        profiles_path = os.path.join(self.output_dir, 'cluster_profiles.csv')
        self.cluster_profiles.to_csv(profiles_path, encoding='utf-8-sig')
        print(f"  âœ… {profiles_path}")

        # 3. ç°‡ç‰¹å¾ç”»åƒï¼ˆZ-scoreï¼‰
        profiles_z_path = os.path.join(self.output_dir, 'cluster_profiles_zscore.csv')
        self.cluster_profiles_z.to_csv(profiles_z_path, encoding='utf-8-sig')
        print(f"  âœ… {profiles_z_path}")

        # 4. èšç±»è¯„ä¼°æŒ‡æ ‡
        metrics_df = pd.DataFrame(self.metrics)
        metrics_path = os.path.join(self.output_dir, 'clustering_metrics.csv')
        metrics_df.to_csv(metrics_path, index=False, encoding='utf-8-sig')
        print(f"  âœ… {metrics_path}")

        # 5. ç®—æ³•æ¯”è¾ƒç»“æœ
        algo_results = []
        for algo, res in self.algorithm_results.items():
            algo_results.append({
                'algorithm': algo,
                'silhouette': res['silhouette'],
                'calinski_harabasz': res['calinski'],
                'davies_bouldin': res['davies_bouldin']
            })
        algo_df = pd.DataFrame(algo_results)
        algo_path = os.path.join(self.output_dir, 'algorithm_comparison.csv')
        algo_df.to_csv(algo_path, index=False, encoding='utf-8-sig')
        print(f"  âœ… {algo_path}")

        # 6. ç°‡å‘½åæ˜ å°„
        names_df = pd.DataFrame([
            {'cluster': k, 'name': v,
             'size': self.cluster_descriptions[k]['size'],
             'percentage': self.cluster_descriptions[k]['pct']}
            for k, v in self.cluster_names.items()
        ])
        names_path = os.path.join(self.output_dir, 'cluster_names.csv')
        names_df.to_csv(names_path, index=False, encoding='utf-8-sig')
        print(f"  âœ… {names_path}")

        print(f"\n  ğŸ“ æ‰€æœ‰ç»“æœå·²ä¿å­˜è‡³: {self.output_dir}/")

        return self

    def run_full_pipeline(self, k_range: tuple = (3, 8), final_k: int = None,
                          scaler_type: str = 'standard'):
        """
        è¿è¡Œå®Œæ•´èšç±»åˆ†ææµæ°´çº¿

        Args:
            k_range: æœç´¢æœ€ä¼˜kçš„èŒƒå›´
            final_k: æŒ‡å®šæœ€ç»ˆä½¿ç”¨çš„kå€¼ï¼ŒNoneåˆ™ä½¿ç”¨è‡ªåŠ¨æ¨è
            scaler_type: æ ‡å‡†åŒ–æ–¹å¼
        """
        # Step 1: é¢„å¤„ç†
        self.preprocess(scaler_type=scaler_type)
        self.analyze_features()
        self.reduce_dimensions()

        # Step 2: ç¡®å®šæœ€ä¼˜k
        self.find_optimal_k(k_range=k_range)

        # å¦‚æœæŒ‡å®šäº†final_kï¼Œä½¿ç”¨æŒ‡å®šå€¼
        if final_k is not None:
            print(f"\n  ğŸ“Œ ä½¿ç”¨æŒ‡å®šèšç±»æ•°: k = {final_k}")
            self.best_k = final_k

        # Step 3: æ‰§è¡Œèšç±»
        self.run_clustering(k=self.best_k)

        # Step 4: åˆ†æç»“æœ
        self.analyze_clusters()

        # Step 5: å•†ä¸šæ´å¯Ÿ
        self.generate_business_insights()

        # Step 6: ä¿å­˜ç»“æœ
        self.save_results()

        # æ‰“å°å®Œæˆä¿¡æ¯
        self._print_summary()

        return self

    def _print_summary(self):
        """æ‰“å°åˆ†ææ‘˜è¦"""
        sil_score = silhouette_score(self.X_scaled, self.best_labels)

        print("\n" + "=" * 70)
        print("                    âœ… èšç±»åˆ†æå®Œæˆï¼")
        print("=" * 70)

        print(f"\n  ğŸ“Š æœ€ç»ˆèšç±»æ•°: {self.best_k}")
        print(f"  ğŸ“ˆ èšç±»ç®—æ³•: {self.best_algorithm}")
        print(f"  ğŸ“‰ è½®å»“ç³»æ•°: {sil_score:.4f}")

        print(f"\n  ğŸ·ï¸ å¸‚åœºç»†åˆ†:")
        for cid, name in self.cluster_names.items():
            size = self.cluster_descriptions[cid]['size']
            pct = self.cluster_descriptions[cid]['pct']
            print(f"     C{cid}: {name} ({size}ä¸ª, {pct:.1f}%)")

        print(f"\n  ğŸ“ è¾“å‡ºæ–‡ä»¶å¤¹: {self.output_dir}/")
        print("     â”œâ”€â”€ clustered_products.csv        (å¸¦èšç±»æ ‡ç­¾çš„å•†å“æ•°æ®)")
        print("     â”œâ”€â”€ cluster_profiles.csv          (å„ç°‡ç‰¹å¾å‡å€¼)")
        print("     â”œâ”€â”€ cluster_profiles_zscore.csv   (å„ç°‡ç‰¹å¾Zåˆ†æ•°)")
        print("     â”œâ”€â”€ cluster_names.csv             (ç°‡å‘½åæ˜ å°„)")
        print("     â”œâ”€â”€ clustering_metrics.csv        (èšç±»è¯„ä¼°æŒ‡æ ‡)")
        print("     â”œâ”€â”€ algorithm_comparison.csv      (ç®—æ³•æ¯”è¾ƒç»“æœ)")
        print("     â”œâ”€â”€ business_insights_report.txt  (å•†ä¸šæ´å¯ŸæŠ¥å‘Š)")
        print("     â”œâ”€â”€ cluster_analysis_dashboard.png(èšç±»åˆ†æé¢æ¿)")
        print("     â”œâ”€â”€ cluster_radar_chart.png       (é›·è¾¾å›¾)")
        print("     â”œâ”€â”€ clustering_results.png        (èšç±»å¯è§†åŒ–)")
        print("     â”œâ”€â”€ correlation_matrix.png        (ç›¸å…³æ€§çƒ­åŠ›å›¾)")
        print("     â”œâ”€â”€ optimal_k_analysis.png        (æœ€ä¼˜Kåˆ†æ)")
        print("     â”œâ”€â”€ dendrogram.png                (å±‚æ¬¡èšç±»æ ‘çŠ¶å›¾)")
        print("     â””â”€â”€ pca_variance.png              (PCAæ–¹å·®è§£é‡Š)")
        print("=" * 70)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    """ä¸»å‡½æ•°"""

    # ==================== é…ç½®å‚æ•° ====================
    DATA_PATH = 'clustering_features_only.csv'  # è¾“å…¥æ•°æ®
    OUTPUT_DIR = 'clustering_results'            # è¾“å‡ºç›®å½•
    K_RANGE = (4, 8)                             # Kå€¼æœç´¢èŒƒå›´
    FINAL_K = 5                                  # æœ€ç»ˆä½¿ç”¨çš„Kå€¼ï¼ˆNone=è‡ªåŠ¨é€‰æ‹©ï¼‰
    SCALER_TYPE = 'standard'                     # æ ‡å‡†åŒ–æ–¹å¼
    # ================================================

    # åˆ›å»ºæµæ°´çº¿
    pipeline = ClusteringPipelineOptimized(
        data_path=DATA_PATH,
        output_dir=OUTPUT_DIR
    )

    # è¿è¡Œå®Œæ•´æµæ°´çº¿
    pipeline.run_full_pipeline(
        k_range=K_RANGE,
        final_k=FINAL_K,
        scaler_type=SCALER_TYPE
    )

    return pipeline


if __name__ == '__main__':
    pipeline = main()
