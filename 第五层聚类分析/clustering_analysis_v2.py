"""
å¨åˆ€å¸‚åœºèšç±»åˆ†æ - ä¼˜åŒ–ç‰ˆ v3.0
================================
æ ¸å¿ƒæ”¹è¿›ï¼š
1. ç‰¹å¾é‡æ„ï¼šæƒ…æ„Ÿ8ç»´â†’2ç»´ï¼Œç§»é™¤å†—ä½™äºŒå€¼ç‰¹å¾
2. K-Prototypesï¼šæ­£ç¡®å¤„ç†æ··åˆæ•°æ®ç±»å‹
3. PCAé™ç»´åå†èšç±»ï¼ˆè¿ç»­ç‰¹å¾éƒ¨åˆ†ï¼‰
4. Gap Statistic + Bootstrapç¨³å®šæ€§éªŒè¯
5. æ›´å¯é çš„Kå€¼é€‰æ‹©é€»è¾‘

ä¾èµ–å®‰è£…ï¼š
    pip install kmodes
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import seaborn as sns
import warnings
import os
from datetime import datetime

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import (
    silhouette_score, silhouette_samples,
    calinski_harabasz_score, davies_bouldin_score,
    adjusted_rand_score
)
from scipy.cluster.hierarchy import dendrogram, linkage

try:
    from kmodes.kprototypes import KPrototypes
    HAS_KMODES = True
except ImportError:
    HAS_KMODES = False
    print("âš ï¸  kmodes æœªå®‰è£…ï¼Œå°†ä½¿ç”¨æ”¹è¿›ç‰ˆ K-Means ä½œä¸ºå¤‡é€‰")
    print("   å®‰è£…å‘½ä»¤: pip install kmodes\n")

warnings.filterwarnings('ignore')

# ============================================================================
# å…¨å±€é…ç½®
# ============================================================================
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

CLUSTER_COLORS = [
    '#E74C3C', '#3498DB', '#2ECC71', '#9B59B6', '#F39C12',
    '#1ABC9C', '#E91E63', '#34495E', '#00BCD4', '#FF5722',
]

# ============================================================================
# ç‰¹å¾å®šä¹‰ï¼ˆv3.0 é‡æ–°åˆ†ç±»ï¼‰
# ============================================================================

# è¿ç»­å‹ç‰¹å¾ï¼ˆé€‚åˆæ ‡å‡†åŒ–+æ¬§æ°è·ç¦»ï¼‰
CONTINUOUS_FEATURES = [
    'log_price_per_piece',   # å•ä»·ï¼ˆä¿ç•™ï¼Œåˆ é™¤ log_price é¿å…å†—ä½™ï¼‰
    'product_rating',
    'log_reviews',
    'log_sales',
    'log_bsr',
    'weighted_rating',
    'discount_rate',
    'blade_size_inch',
    'log_days_on_market',
    'verified_purchase_rate',
    'avg_helpful_votes',
    'set_pieces',            # æ¯” is_set ä¿¡æ¯é‡æ›´å¤§
    # ä»¥ä¸‹ä¸¤åˆ—ç”±æƒ…æ„Ÿ8ç»´åˆå¹¶è€Œæ¥ï¼ˆåœ¨ preprocess ä¸­ç”Ÿæˆï¼‰
    'sentiment_avg',
    'sentiment_std',
    'positive_ratio',
    'bullet_count',
    'image_count',
]

# åˆ†ç±»/äºŒå€¼å‹ç‰¹å¾ï¼ˆé€‚åˆæ±‰æ˜è·ç¦»ï¼ŒK-Prototypesä¸“ç”¨ï¼‰
CATEGORICAL_FEATURES = [
    'is_fba',
    'has_aplus',
    'brand_tier_encoded',
    'is_damascus',
    'is_high_carbon',
    'is_german_steel',
    'is_japanese_steel',
    'is_ceramic',
    'is_chef_knife',
    'is_santoku',
    'is_steak_knife',
    'is_cleaver',
    'is_paring',
    'is_professional',
    'is_gift',
]

# éœ€è¦åˆ é™¤çš„å†—ä½™ç‰¹å¾
DROP_FEATURES = [
    'negative_ratio',         # = 1 - positive_ratio
    'bert_sentiment_mean',    # â‰ˆ positive_ratio
    'log_price',              # ä¸ log_price_per_piece ç›¸å…³ï¼Œå¥—è£…åœºæ™¯ä¸‹åè€…æ›´å‡†ç¡®
    'is_set',                 # set_pieces > 0 å³ä¸ºå¥—è£…ï¼Œä¿¡æ¯é‡å¤
    'has_block',              # ä¸ is_set/set_pieces å¼ºç›¸å…³
    # æƒ…æ„Ÿå­ç»´åº¦ï¼šåˆå¹¶ä¸º sentiment_avg + sentiment_std
    'aspect_sharpness_sentiment',
    'aspect_quality_sentiment',
    'aspect_durability_sentiment',
    'aspect_handle_sentiment',
    'aspect_value_sentiment',
    'aspect_rust_sentiment',
    'aspect_appearance_sentiment',
    'aspect_balance_sentiment',
    'aspect_sentiment_mean',
]


# ============================================================================
# ä¸»ç±»
# ============================================================================

class ClusteringPipelineV3:
    """
    å¨åˆ€å¸‚åœºèšç±»åˆ†æ v3.0

    æ ¸å¿ƒæ”¹è¿›ï¼š
    - æ··åˆæ•°æ®ç±»å‹æ­£ç¡®å¤„ç†ï¼ˆK-Prototypesï¼‰
    - æƒ…æ„Ÿç‰¹å¾é™ç»´èšåˆ
    - Gap Statistic + Bootstrap ç¨³å®šæ€§åŒé‡éªŒè¯
    - æ›´ä¿å®ˆçš„èšç±»ç»“è®ºè¾“å‡º
    """

    def __init__(self, data_path='clustering_features_only.csv',
                 output_dir='clustering_results_v3'):
        self.data_path = data_path
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)

        self.df_raw = pd.read_csv(data_path)
        self.asin_col = 'asin' if 'asin' in self.df_raw.columns else self.df_raw.columns[0]
        self.asins = self.df_raw[self.asin_col].values

        # è¿è¡Œæ—¶å˜é‡
        self.df_feat = None          # é¢„å¤„ç†åç‰¹å¾ DataFrame
        self.X_cont_scaled = None    # æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾
        self.X_cat = None            # åŸå§‹åˆ†ç±»ç‰¹å¾ï¼ˆæ•´æ•°ï¼‰
        self.X_combined = None       # æ‹¼æ¥åï¼ˆK-Meanså¤‡é€‰ç”¨ï¼‰
        self.X_for_cluster = None    # å®é™…ä¼ å…¥èšç±»å™¨çš„æ•°æ®
        self.cont_cols_final = None  # æœ€ç»ˆæœ‰æ•ˆè¿ç»­åˆ—å
        self.cat_cols_final = None   # æœ€ç»ˆæœ‰æ•ˆåˆ†ç±»åˆ—å
        self.scaler = None
        self.pca = None
        self.X_pca = None
        self.X_tsne = None
        self.best_k = None
        self.best_labels = None
        self.cluster_profiles = None
        self.cluster_profiles_z = None
        self.cluster_names = None
        self.cluster_descriptions = None
        self.stability_results = {}

        print("=" * 70)
        print("       å¨åˆ€å¸‚åœºèšç±»åˆ†æ v3.0 (æ··åˆæ•°æ®ç±»å‹ä¼˜åŒ–ç‰ˆ)")
        print("=" * 70)
        print(f"\nğŸ“Š åŸå§‹æ•°æ®: {len(self.df_raw)} æ ·æœ¬ Ã— {len(self.df_raw.columns)-1} ç‰¹å¾")
        print(f"ğŸ”§ åç«¯æ¨¡å¼: {'K-Prototypes' if HAS_KMODES else 'K-Means (æ”¹è¿›ç‰ˆ)'}")

    # -------------------------------------------------------------------------
    # Step 1: ç‰¹å¾å·¥ç¨‹
    # -------------------------------------------------------------------------
    def preprocess(self):
        print("\n" + "=" * 70)
        print("[Step 1] ç‰¹å¾å·¥ç¨‹ä¸é¢„å¤„ç†")
        print("=" * 70)

        df = self.df_raw.drop(columns=[self.asin_col]).copy()

        # â”€â”€ 1.1 ç”Ÿæˆåˆå¹¶æƒ…æ„Ÿç‰¹å¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        aspect_cols = [c for c in DROP_FEATURES
                       if c.startswith('aspect_') and c in df.columns]
        if aspect_cols:
            df['sentiment_avg'] = df[aspect_cols].mean(axis=1)
            df['sentiment_std'] = df[aspect_cols].std(axis=1)
            print(f"\n  âœ… æƒ…æ„Ÿç»´åº¦åˆå¹¶: {len(aspect_cols)} åˆ— â†’ sentiment_avg + sentiment_std")

        # â”€â”€ 1.2 åˆ é™¤å†—ä½™ç‰¹å¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        drop_actual = [c for c in DROP_FEATURES if c in df.columns]
        df.drop(columns=drop_actual, inplace=True)
        print(f"  âœ… åˆ é™¤å†—ä½™ç‰¹å¾ {len(drop_actual)} ä¸ª: {drop_actual[:6]}{'...' if len(drop_actual)>6 else ''}")

        # â”€â”€ 1.3 ç¡®å®šå®é™…å¯ç”¨åˆ— â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.cont_cols_final = [c for c in CONTINUOUS_FEATURES if c in df.columns]
        self.cat_cols_final  = [c for c in CATEGORICAL_FEATURES  if c in df.columns]

        print(f"\n  ğŸ“Š æœ€ç»ˆç‰¹å¾æ„æˆ:")
        print(f"     è¿ç»­å‹: {len(self.cont_cols_final)} ä¸ª")
        print(f"     åˆ†ç±»å‹: {len(self.cat_cols_final)} ä¸ª")
        print(f"     åˆè®¡:   {len(self.cont_cols_final)+len(self.cat_cols_final)} ä¸ª (åŸå§‹42â†’ä¼˜åŒ–å)")

        # â”€â”€ 1.4 ç¼ºå¤±å€¼/æ— ç©·å€¼å¤„ç† â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        for col in self.cont_cols_final:
            col_data = df[col].replace([np.inf, -np.inf], np.nan)
            if col_data.isna().sum() > 0:
                df[col] = col_data.fillna(col_data.median())
            else:
                df[col] = col_data

        for col in self.cat_cols_final:
            df[col] = df[col].fillna(0).astype(int)

        self.df_feat = df

        # â”€â”€ 1.5 æ ‡å‡†åŒ–è¿ç»­ç‰¹å¾ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.scaler = StandardScaler()
        self.X_cont_scaled = self.scaler.fit_transform(df[self.cont_cols_final].values)
        self.X_cat = df[self.cat_cols_final].values.astype(int)

        # æ‹¼æ¥ï¼ˆK-Meanså¤‡é€‰ æˆ– PCAå¯è§†åŒ– ç”¨ï¼‰
        self.X_combined = np.hstack([self.X_cont_scaled, self.X_cat])

        print(f"\n  âœ… æ ‡å‡†åŒ–å®Œæˆ (StandardScaler on {len(self.cont_cols_final)} è¿ç»­ç‰¹å¾)")

        # â”€â”€ 1.6 ç›¸å…³æ€§æ£€æŸ¥ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self._check_remaining_correlation()

        return self

    def _check_remaining_correlation(self):
        """æ£€æŸ¥å¤„ç†åè¿ç»­ç‰¹å¾é—´çš„æ®‹ä½™ç›¸å…³æ€§"""
        df_cont = pd.DataFrame(self.X_cont_scaled, columns=self.cont_cols_final)
        corr = df_cont.corr().abs()
        high_pairs = []
        for i in range(len(self.cont_cols_final)):
            for j in range(i+1, len(self.cont_cols_final)):
                if corr.iloc[i, j] > 0.75:
                    high_pairs.append((self.cont_cols_final[i],
                                       self.cont_cols_final[j],
                                       corr.iloc[i, j]))
        if high_pairs:
            print(f"\n  âš ï¸  æ®‹ä½™é«˜ç›¸å…³ç‰¹å¾å¯¹ (|r|>0.75):")
            for f1, f2, r in sorted(high_pairs, key=lambda x: -x[2])[:5]:
                print(f"     {f1} â†” {f2}: r={r:.3f}")
        else:
            print("  âœ… æ— é«˜åº¦ç›¸å…³ç‰¹å¾å¯¹æ®‹ç•™")

    # -------------------------------------------------------------------------
    # Step 2: é™ç»´
    # -------------------------------------------------------------------------
    def reduce_dimensions(self):
        print("\n" + "=" * 70)
        print("[Step 2] é™ç»´åˆ†æ")
        print("=" * 70)

        # â”€â”€ PCAï¼ˆä»…å¯¹è¿ç»­ç‰¹å¾ï¼Œç”¨äºèšç±»å’Œå¯è§†åŒ–ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # è‡ªåŠ¨ç¡®å®šç»´æ•°ï¼ˆä¿ç•™90%æ–¹å·®ï¼Œä½†ä¸Šé™20ç»´ï¼‰
        pca_full = PCA(random_state=42)
        pca_full.fit(self.X_cont_scaled)
        cumvar = np.cumsum(pca_full.explained_variance_ratio_)
        n_90 = int(np.argmax(cumvar >= 0.90)) + 1
        n_components = min(n_90, 20, self.X_cont_scaled.shape[1])

        self.pca = PCA(n_components=n_components, random_state=42)
        self.X_pca = self.pca.fit_transform(self.X_cont_scaled)

        print(f"\n  ğŸ“‰ PCA ç»“æœ (è¿ç»­ç‰¹å¾):")
        print(f"     åŸå§‹è¿ç»­ç»´æ•°: {self.X_cont_scaled.shape[1]}")
        print(f"     ä¿ç•™ç»´æ•° (â‰¥90%æ–¹å·®): {n_components}")
        print(f"     å®é™…æ–¹å·®è§£é‡Š: {cumvar[n_components-1]*100:.1f}%")
        print(f"     å‰2PCæ–¹å·®: {cumvar[1]*100:.1f}%")

        # ä¿å­˜æ–¹å·®å›¾
        self._plot_pca_variance(pca_full, cumvar, n_components)

        # â”€â”€ t-SNEï¼ˆå…¨ç‰¹å¾ç”¨äºå¯è§†åŒ–ï¼ŒPCAé¢„å¤„ç†åŠ é€Ÿï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print("\n  ğŸ”„ t-SNE é™ç»´ (ç”¨äºå¯è§†åŒ–)...")
        perplexity = min(30, len(self.X_combined) - 1)
        # å…ˆç”¨PCAå‹åˆ°50ç»´åŠ é€Ÿt-SNE
        n_pre = min(50, self.X_combined.shape[1])
        pca_pre = PCA(n_components=n_pre, random_state=42)
        X_pre = pca_pre.fit_transform(self.X_combined)

        try:
            tsne = TSNE(n_components=2, perplexity=perplexity,
                        random_state=42, max_iter=1000)
        except TypeError:
            tsne = TSNE(n_components=2, perplexity=perplexity,
                        random_state=42, n_iter=1000)
        self.X_tsne = tsne.fit_transform(X_pre)
        print("  âœ… t-SNE å®Œæˆ")

        return self

    def _plot_pca_variance(self, pca_full, cumvar, n_selected):
        n_show = min(20, len(cumvar))
        fig, axes = plt.subplots(1, 2, figsize=(13, 4))

        ax = axes[0]
        ax.bar(range(1, n_show+1), pca_full.explained_variance_ratio_[:n_show],
               color='#3498db', alpha=0.8, edgecolor='white')
        ax.axvline(x=n_selected, color='red', linestyle='--', linewidth=2,
                   label=f'Selected: {n_selected}')
        ax.set_xlabel('Principal Component'); ax.set_ylabel('Explained Variance Ratio')
        ax.set_title('PCA Explained Variance (Continuous Features)', fontweight='bold')
        ax.legend(); ax.set_xticks(range(1, n_show+1))

        ax = axes[1]
        ax.plot(range(1, n_show+1), cumvar[:n_show], 'o-', color='#e74c3c', linewidth=2)
        ax.axvline(x=n_selected, color='red', linestyle='--', linewidth=2)
        ax.axhline(y=0.90, color='gray', linestyle='--', alpha=0.7, label='90%')
        ax.axhline(y=0.95, color='gray', linestyle=':', alpha=0.7, label='95%')
        ax.fill_between(range(1, n_show+1), cumvar[:n_show], alpha=0.2, color='#e74c3c')
        ax.set_xlabel('Number of Components'); ax.set_ylabel('Cumulative Explained Variance')
        ax.set_title('Cumulative Variance Explained', fontweight='bold')
        ax.legend(loc='lower right'); ax.set_ylim(0, 1.05); ax.set_xticks(range(1, n_show+1))

        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'pca_variance_v3.png'), dpi=150, bbox_inches='tight')
        plt.close()

    # -------------------------------------------------------------------------
    # Step 3: Kå€¼é€‰æ‹©ï¼ˆGap Statistic + ä¼ ç»ŸæŒ‡æ ‡ + Bootstrapç¨³å®šæ€§ï¼‰
    # -------------------------------------------------------------------------
    def find_optimal_k(self, k_range=(2, 9), n_gap_refs=15, n_bootstrap=20):
        print("\n" + "=" * 70)
        print("[Step 3] æœ€ä¼˜Kå€¼ç¡®å®šï¼ˆå¤šé‡éªŒè¯ï¼‰")
        print("=" * 70)

        k_min, k_max = k_range
        k_values = list(range(k_min, k_max + 1))

        # èšç±»æ‰€ç”¨æ•°æ®ï¼šPCAé™ç»´åè¿ç»­ç‰¹å¾ + åŸå§‹åˆ†ç±»ç‰¹å¾
        # ï¼ˆæ­¤å¤„ç”¨äºKå€¼é€‰æ‹©ï¼Œå®é™…èšç±»æ ¹æ®æ˜¯å¦æœ‰kmodeså†³å®šï¼‰
        X_eval = np.hstack([self.X_pca, self.X_cat])

        # â”€â”€ 3.1 ä¼ ç»ŸæŒ‡æ ‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        metrics = {'k': k_values, 'inertia': [], 'silhouette': [],
                   'calinski': [], 'davies_bouldin': []}

        print(f"\n  ğŸ” ä¼ ç»ŸæŒ‡æ ‡è¯„ä¼° (k = {k_min} ~ {k_max})")
        print("  " + "-" * 65)
        print(f"  {'K':<4} {'Inertia':<11} {'Silhouette':<12} {'CH Index':<11} {'DB Index'}")
        print("  " + "-" * 65)

        all_labels = {}
        for k in k_values:
            km = KMeans(n_clusters=k, init='k-means++', n_init=30,
                        max_iter=500, random_state=42)
            labels = km.fit_predict(X_eval)
            all_labels[k] = labels

            sil = silhouette_score(X_eval, labels)
            ch  = calinski_harabasz_score(X_eval, labels)
            db  = davies_bouldin_score(X_eval, labels)

            metrics['inertia'].append(km.inertia_)
            metrics['silhouette'].append(sil)
            metrics['calinski'].append(ch)
            metrics['davies_bouldin'].append(db)
            print(f"  {k:<4} {km.inertia_:<11.1f} {sil:<12.4f} {ch:<11.1f} {db:.4f}")

        print("  " + "-" * 65)

        # â”€â”€ 3.2 Gap Statistic â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(f"\n  ğŸ“ Gap Statistic è®¡ç®— (n_refs={n_gap_refs})...")
        gaps, gap_stds = self._gap_statistic(X_eval, k_values, n_gap_refs)

        print(f"\n  {'K':<4} {'Gap':<10} {'Std':<10} {'Gap(k)-Gap(k+1)+Std(k+1)'}")
        print("  " + "-" * 50)
        gap_k = None
        for i, k in enumerate(k_values):
            if i < len(k_values) - 1:
                criterion = gaps[i] - gaps[i+1] + gap_stds[i+1]
                flag = " â† æ¨è" if criterion >= 0 and gap_k is None else ""
                if criterion >= 0 and gap_k is None:
                    gap_k = k
                print(f"  {k:<4} {gaps[i]:<10.4f} {gap_stds[i]:<10.4f} {criterion:.4f}{flag}")
            else:
                print(f"  {k:<4} {gaps[i]:<10.4f} {gap_stds[i]:<10.4f} -")

        if gap_k is None:
            gap_k = k_values[np.argmax(gaps)]
        print(f"\n  ğŸ¯ Gap Statistic æ¨è: k = {gap_k}")

        # â”€â”€ 3.3 Bootstrap ç¨³å®šæ€§ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        print(f"\n  ğŸ”„ Bootstrap ç¨³å®šæ€§æ£€éªŒ (n={n_bootstrap})...")
        stability_scores = {}
        print(f"  {'K':<4} {'ARIå‡å€¼':<10} {'ARIæ ‡å‡†å·®':<12} {'è¯„çº§'}")
        print("  " + "-" * 45)

        for k in k_values:
            mean_ari, std_ari = self._bootstrap_stability(X_eval, k, n_bootstrap)
            stability_scores[k] = (mean_ari, std_ari)
            if mean_ari >= 0.85:
                grade = "ğŸŸ¢ ç¨³å®š"
            elif mean_ari >= 0.65:
                grade = "ğŸŸ¡ ä¸­ç­‰"
            else:
                grade = "ğŸ”´ ä¸ç¨³å®š"
            print(f"  {k:<4} {mean_ari:<10.3f} {std_ari:<12.3f} {grade}")

        self.stability_results = stability_scores

        # â”€â”€ 3.4 ç»¼åˆå†³ç­– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        self.best_k = self._decide_k(
            k_values, metrics, gaps, gap_stds, stability_scores, gap_k
        )

        print(f"\n  {'='*50}")
        print(f"  ğŸ† æœ€ç»ˆæ¨è K = {self.best_k}")
        print(f"  {'='*50}")

        # ä¿å­˜è¯„ä¼°å›¾
        self._plot_k_selection(k_values, metrics, gaps, gap_stds, stability_scores)

        self.metrics = metrics
        self.all_labels_eval = all_labels
        return self

    def _gap_statistic(self, X, k_values, n_refs):
        gaps, gap_stds = [], []
        rng = np.random.RandomState(42)

        for k in k_values:
            km = KMeans(n_clusters=k, n_init=10, random_state=42)
            km.fit(X)
            log_wk = np.log(km.inertia_)

            ref_log_wks = []
            for _ in range(n_refs):
                X_ref = rng.uniform(X.min(axis=0), X.max(axis=0), X.shape)
                km_ref = KMeans(n_clusters=k, n_init=5, random_state=42)
                km_ref.fit(X_ref)
                ref_log_wks.append(np.log(km_ref.inertia_))

            gap = np.mean(ref_log_wks) - log_wk
            std = np.std(ref_log_wks) * np.sqrt(1 + 1/n_refs)
            gaps.append(gap)
            gap_stds.append(std)

        return gaps, gap_stds

    def _bootstrap_stability(self, X, k, n_bootstrap):
        base_km = KMeans(n_clusters=k, n_init=20, random_state=42)
        base_labels = base_km.fit_predict(X)
        ari_scores = []

        rng = np.random.RandomState(42)
        for i in range(n_bootstrap):
            # å…¼å®¹æ‰€æœ‰ sklearn ç‰ˆæœ¬ï¼šæ‰‹åŠ¨ç”Ÿæˆ bootstrap ç´¢å¼•
            idx = rng.choice(len(X), size=len(X), replace=True)
            X_boot = X[idx]
            km_b = KMeans(n_clusters=k, n_init=5, random_state=i)
            boot_labels = km_b.fit_predict(X_boot)
            ari = adjusted_rand_score(base_labels[idx], boot_labels)
            ari_scores.append(ari)

        return float(np.mean(ari_scores)), float(np.std(ari_scores))

    def _decide_k(self, k_values, metrics, gaps, gap_stds, stability, gap_k):
        """ç»¼åˆå¤šä¸ªæŒ‡æ ‡çš„Kå€¼å†³ç­–é€»è¾‘"""
        scores = np.zeros(len(k_values))

        # â”€â”€ è½®å»“ç³»æ•°å½’ä¸€åŒ–ï¼ˆæƒé‡40%ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        sil = np.array(metrics['silhouette'])
        sil_n = (sil - sil.min()) / (sil.max() - sil.min() + 1e-8)
        scores += 0.40 * sil_n

        # â”€â”€ Gap Statisticï¼šç”¨åˆ¤å®šå‡†åˆ™æ‰“åˆ†ï¼Œè€ŒéåŸå§‹å€¼å½’ä¸€åŒ– â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # å‡†åˆ™ï¼šgap(k) >= gap(k+1) - std(k+1)  =>  è¯¥kæ˜¯å€™é€‰æ‹ç‚¹
        # Gapå•è°ƒé€’å¢æ—¶åŸå§‹å€¼å½’ä¸€åŒ–ä¼šé”™è¯¯åœ°ç»™æœ€å¤§kæ»¡åˆ†ï¼Œæ”¹ç”¨å‡†åˆ™å¾—åˆ†
        gap_criterion_score = np.zeros(len(k_values))
        for i in range(len(k_values) - 1):
            criterion = gaps[i] - gaps[i+1] + gap_stds[i+1]
            # criterionè¶Šå¤§è¯´æ˜è¶Šæ˜¯æ‹ç‚¹ï¼Œæ­£å€¼æ‰æœ‰æ„ä¹‰
            gap_criterion_score[i] = max(0.0, criterion)
        # å½’ä¸€åŒ–å‡†åˆ™åˆ†
        if gap_criterion_score.max() > 0:
            gap_n = gap_criterion_score / gap_criterion_score.max()
        else:
            gap_n = gap_criterion_score
        scores += 0.30 * gap_n

        # â”€â”€ Bootstrapç¨³å®šæ€§å½’ä¸€åŒ–ï¼ˆæƒé‡30%ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        stab = np.array([stability[k][0] for k in k_values])
        stab_n = (stab - stab.min()) / (stab.max() - stab.min() + 1e-8)
        scores += 0.30 * stab_n

        best_idx = int(np.argmax(scores))
        best_k = k_values[best_idx]

        print(f"\n  ğŸ“Š ç»¼åˆè¯„åˆ† (è½®å»“40% + Gapå‡†åˆ™30% + ç¨³å®šæ€§30%):")
        for i, k in enumerate(k_values):
            marker = " â† æœ€ä¼˜" if k == best_k else ""
            print(f"     k={k}: {scores[i]:.4f}  "
                  f"[sil={sil[i]:.3f}, gap_crit={gap_criterion_score[i]:.4f}, "
                  f"ari={stab[i]:.3f}]{marker}")

        return best_k

    def _plot_k_selection(self, k_values, metrics, gaps, gap_stds, stability):
        fig = plt.figure(figsize=(16, 12))
        gs = gridspec.GridSpec(3, 2, figure=fig, hspace=0.4, wspace=0.35)

        colors_bar = ['#e74c3c' if k == self.best_k else '#3498db' for k in k_values]

        # 1. è‚˜éƒ¨
        ax1 = fig.add_subplot(gs[0, 0])
        ax1.plot(k_values, metrics['inertia'], 'bo-', lw=2, ms=7)
        ax1.axvline(x=self.best_k, color='red', ls='--', lw=2, label=f'Best k={self.best_k}')
        ax1.set_title('Elbow Method (SSE)', fontweight='bold')
        ax1.set_xlabel('K'); ax1.set_ylabel('Inertia')
        ax1.legend(); ax1.grid(True, alpha=0.3); ax1.set_xticks(k_values)

        # 2. è½®å»“ç³»æ•°
        ax2 = fig.add_subplot(gs[0, 1])
        bars = ax2.bar(k_values, metrics['silhouette'], color=colors_bar, edgecolor='white', lw=1.5)
        for b, v in zip(bars, metrics['silhouette']):
            ax2.text(b.get_x()+b.get_width()/2, b.get_height()+0.001,
                    f'{v:.3f}', ha='center', va='bottom', fontsize=8)
        ax2.set_title('Silhouette Score', fontweight='bold')
        ax2.set_xlabel('K'); ax2.set_ylabel('Score')
        ax2.grid(True, alpha=0.3, axis='y'); ax2.set_xticks(k_values)

        # 3. Gap Statistic
        ax3 = fig.add_subplot(gs[1, 0])
        ax3.errorbar(k_values, gaps, yerr=gap_stds, fmt='go-', lw=2, ms=7,
                    ecolor='gray', capsize=4, label='Gap Â± Std')
        ax3.axvline(x=self.best_k, color='red', ls='--', lw=2)
        ax3.set_title('Gap Statistic', fontweight='bold')
        ax3.set_xlabel('K'); ax3.set_ylabel('Gap Value')
        ax3.legend(); ax3.grid(True, alpha=0.3); ax3.set_xticks(k_values)

        # 4. Bootstrap ç¨³å®šæ€§
        ax4 = fig.add_subplot(gs[1, 1])
        means = [stability[k][0] for k in k_values]
        stds  = [stability[k][1] for k in k_values]
        ax4.errorbar(k_values, means, yerr=stds, fmt='mo-', lw=2, ms=7,
                    ecolor='gray', capsize=4)
        ax4.axhline(y=0.85, color='green', ls='--', alpha=0.7, label='ç¨³å®šé˜ˆå€¼(0.85)')
        ax4.axhline(y=0.65, color='orange', ls='--', alpha=0.7, label='ä¸­ç­‰é˜ˆå€¼(0.65)')
        ax4.axvline(x=self.best_k, color='red', ls='--', lw=2)
        ax4.set_title('Bootstrap Stability (ARI)', fontweight='bold')
        ax4.set_xlabel('K'); ax4.set_ylabel('Adjusted Rand Index')
        ax4.legend(fontsize=8); ax4.grid(True, alpha=0.3); ax4.set_xticks(k_values)
        ax4.set_ylim(0, 1.05)

        # 5. CH Index
        ax5 = fig.add_subplot(gs[2, 0])
        ax5.plot(k_values, metrics['calinski'], 'co-', lw=2, ms=7)
        ax5.axvline(x=self.best_k, color='red', ls='--', lw=2)
        ax5.set_title('Calinski-Harabasz Index (Higher=Better)', fontweight='bold')
        ax5.set_xlabel('K'); ax5.set_ylabel('CH Index')
        ax5.grid(True, alpha=0.3); ax5.set_xticks(k_values)

        # 6. DB Index
        ax6 = fig.add_subplot(gs[2, 1])
        ax6.plot(k_values, metrics['davies_bouldin'], 'yo-', lw=2, ms=7,
                color='#e67e22')
        ax6.axvline(x=self.best_k, color='red', ls='--', lw=2)
        ax6.set_title('Davies-Bouldin Index (Lower=Better)', fontweight='bold')
        ax6.set_xlabel('K'); ax6.set_ylabel('DB Index')
        ax6.grid(True, alpha=0.3); ax6.set_xticks(k_values)

        fig.suptitle(f'Optimal K Analysis v3.0 â€” Best: k={self.best_k}',
                    fontsize=14, fontweight='bold')
        plt.savefig(os.path.join(self.output_dir, 'optimal_k_v3.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()
        print(f"\n  ğŸ“Š Kå€¼åˆ†æå›¾å·²ä¿å­˜")

    # -------------------------------------------------------------------------
    # Step 4: æ‰§è¡Œèšç±»
    # -------------------------------------------------------------------------
    def run_clustering(self, k=None):
        if k is None:
            k = self.best_k
        else:
            self.best_k = k

        print("\n" + "=" * 70)
        print(f"[Step 4] æ‰§è¡Œèšç±» (k = {k})")
        print("=" * 70)

        X_pca_cat = np.hstack([self.X_pca, self.X_cat])

        if HAS_KMODES:
            self._run_kprototypes(k)
        else:
            self._run_kmeans_improved(k, X_pca_cat)

        # è´¨é‡æŠ¥å‘Š
        sil = silhouette_score(X_pca_cat, self.best_labels)
        ch  = calinski_harabasz_score(X_pca_cat, self.best_labels)
        db  = davies_bouldin_score(X_pca_cat, self.best_labels)

        print(f"\n  ğŸ“Š èšç±»è´¨é‡æŒ‡æ ‡:")
        print(f"     è½®å»“ç³»æ•°:  {sil:.4f}  {'ğŸŸ¢ è‰¯å¥½' if sil>0.3 else 'ğŸŸ¡ ä¸­ç­‰' if sil>0.15 else 'ğŸ”´ è¾ƒå·®'}")
        print(f"     CH Index:  {ch:.1f}")
        print(f"     DB Index:  {db:.4f}")

        # Bootstrapæœ€ç»ˆç¨³å®šæ€§
        mean_ari, std_ari = self.stability_results.get(k, (None, None))
        if mean_ari is not None:
            print(f"     ç¨³å®šæ€§ARI: {mean_ari:.3f} Â± {std_ari:.3f}  "
                  f"{'ğŸŸ¢ ç¨³å®š' if mean_ari>=0.85 else 'ğŸŸ¡ ä¸­ç­‰' if mean_ari>=0.65 else 'ğŸ”´ ä¸ç¨³å®š'}")

        if sil < 0.15:
            print("\n  âš ï¸  è­¦å‘Šï¼šè½®å»“ç³»æ•° < 0.15ï¼Œèšç±»ç»“æ„è¾ƒå¼±")
            print("     å»ºè®®ï¼šèšç±»ç»“è®ºä»…ç”¨äºæ¢ç´¢æ€§å‚è€ƒï¼Œå‹¿ä½œä¸ºå¼ºå†³ç­–ä¾æ®")

        # ç°‡å¤§å°
        sizes = pd.Series(self.best_labels).value_counts().sort_index()
        print(f"\n  ğŸ“Š ç°‡å¤§å°:")
        for cid, sz in sizes.items():
            pct = sz / len(self.best_labels) * 100
            print(f"     C{cid}: {sz:>4} ({pct:>5.1f}%) {'â–ˆ'*int(pct/3)}")

        self._plot_clustering_scatter()
        return self

    def _run_kprototypes(self, k):
        """K-Prototypesï¼šæ­£ç¡®å¤„ç†è¿ç»­+åˆ†ç±»æ··åˆæ•°æ®"""
        print("\n  ğŸ”§ ä½¿ç”¨ K-Prototypes (è¿ç»­+åˆ†ç±»æ··åˆ)")
        # åŸå§‹è¿ç»­ç‰¹å¾ï¼ˆæ ‡å‡†åŒ–ï¼‰+ åŸå§‹åˆ†ç±»ç‰¹å¾
        X_kp = np.hstack([self.X_cont_scaled, self.X_cat.astype(float)])
        cat_idx = list(range(self.X_cont_scaled.shape[1],
                              self.X_cont_scaled.shape[1] + self.X_cat.shape[1]))

        kp = KPrototypes(n_clusters=k, init='Cao', n_init=10,
                         random_state=42, verbose=0)
        self.best_labels = kp.fit_predict(X_kp, categorical=cat_idx)
        self.kp_model = kp
        print("  âœ… K-Prototypes å®Œæˆ")

    def _run_kmeans_improved(self, k, X):
        """æ”¹è¿›ç‰ˆ K-Meansï¼ˆkmodesä¸å¯ç”¨æ—¶çš„å¤‡é€‰ï¼‰"""
        print("\n  ğŸ”§ ä½¿ç”¨æ”¹è¿›ç‰ˆ K-Means (PCAé™ç»´åè¿ç»­+åˆ†ç±»æ‹¼æ¥)")
        km = KMeans(n_clusters=k, init='k-means++', n_init=50,
                    max_iter=500, random_state=42)
        self.best_labels = km.fit_predict(X)
        self.km_model = km
        print("  âœ… K-Means å®Œæˆ")

    def _plot_clustering_scatter(self):
        k = len(np.unique(self.best_labels))
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        for ax, (X2d, xlabel, ylabel, title) in zip(axes, [
            (self.X_tsne, 't-SNE Dim 1', 't-SNE Dim 2', 't-SNE Visualization'),
            (self.X_pca[:, :2],
             f'PC1 ({self.pca.explained_variance_ratio_[0]*100:.1f}%)',
             f'PC2 ({self.pca.explained_variance_ratio_[1]*100:.1f}%)',
             'PCA Visualization (PC1 vs PC2)')
        ]):
            for i in range(k):
                m = self.best_labels == i
                ax.scatter(X2d[m, 0], X2d[m, 1],
                          c=CLUSTER_COLORS[i % len(CLUSTER_COLORS)],
                          label=f'C{i} (n={m.sum()})',
                          alpha=0.6, s=45, edgecolors='white', lw=0.5)
            ax.set_xlabel(xlabel); ax.set_ylabel(ylabel)
            ax.set_title(title, fontweight='bold')
            ax.legend(fontsize=8)

        fig.suptitle(f'Clustering Results (k={k}) â€” v3.0', fontsize=13, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'clustering_scatter_v3.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()
        print("  ğŸ“Š èšç±»æ•£ç‚¹å›¾å·²ä¿å­˜")

    # -------------------------------------------------------------------------
    # Step 5: ç°‡åˆ†æ
    # -------------------------------------------------------------------------
    def analyze_clusters(self):
        print("\n" + "=" * 70)
        print("[Step 5] ç°‡ç‰¹å¾åˆ†æ")
        print("=" * 70)

        # ç”¨åŸå§‹è¿ç»­ç‰¹å¾ï¼ˆæœªæ ‡å‡†åŒ–ï¼‰åšç”»åƒï¼Œæ–¹ä¾¿ä¸šåŠ¡è§£è¯»
        all_cols = self.cont_cols_final + self.cat_cols_final
        df_a = self.df_feat[all_cols].copy()
        df_a['cluster'] = self.best_labels

        profiles     = df_a.groupby('cluster')[all_cols].mean()
        overall_mean = df_a[all_cols].mean()
        overall_std  = df_a[all_cols].std()
        profiles_z   = (profiles - overall_mean) / (overall_std + 1e-8)

        self.cluster_profiles   = profiles
        self.cluster_profiles_z = profiles_z

        n_clusters = len(np.unique(self.best_labels))
        cluster_sizes = df_a['cluster'].value_counts().sort_index()
        cluster_descriptions = {}

        print("\n  ğŸ” å„ç°‡æ˜¾è‘—ç‰¹å¾ (|z| > 0.5):")
        print("  " + "-" * 60)

        for cid in range(n_clusters):
            z = profiles_z.loc[cid]
            high = z[z >  0.5].sort_values(ascending=False)
            low  = z[z < -0.5].sort_values()
            sz   = cluster_sizes[cid]
            pct  = sz / len(df_a) * 100

            print(f"\n  ã€C{cid}ã€‘ {sz}ä¸ª ({pct:.1f}%)")
            if len(high): print(f"    â†‘ {dict(list(high.head(5).items()))}")
            if len(low):  print(f"    â†“ {dict(list(low.head(5).items()))}")

            cluster_descriptions[cid] = {
                'size': sz, 'pct': pct,
                'high': high.head(5).to_dict(),
                'low':  low.head(5).to_dict()
            }

        self.cluster_descriptions = cluster_descriptions
        self.df_clustered = df_a

        # å¯è§†åŒ–
        self._plot_analysis_dashboard(n_clusters)
        self._plot_feature_heatmap(n_clusters)
        self._plot_radar(n_clusters)
        self._plot_silhouette(n_clusters)

        return self

    def _plot_analysis_dashboard(self, n_clusters):
        fig, axes = plt.subplots(2, 2, figsize=(14, 11))

        # 1. é¥¼å›¾
        ax = axes[0, 0]
        sizes = [self.cluster_descriptions[i]['size'] for i in range(n_clusters)]
        colors_pie = [CLUSTER_COLORS[i % len(CLUSTER_COLORS)] for i in range(n_clusters)]
        ax.pie(sizes,
               labels=[f"C{i}\n({self.cluster_descriptions[i]['pct']:.1f}%)"
                       for i in range(n_clusters)],
               autopct='%1.0f%%', colors=colors_pie,
               explode=[0.03]*n_clusters, textprops={'fontsize': 9})
        ax.set_title('Cluster Size Distribution', fontweight='bold')

        # 2. å…³é”®è¿ç»­ç‰¹å¾ç®±çº¿å›¾
        ax = axes[0, 1]
        key = ['log_price_per_piece', 'product_rating', 'log_sales', 'positive_ratio']
        key = [c for c in key if c in self.cont_cols_final][:3]
        if key:
            melt = self.df_clustered[['cluster']+key].melt(
                id_vars='cluster', var_name='Feature', value_name='Value')
            sns.boxplot(data=melt, x='Feature', y='Value', hue='cluster',
                       palette=CLUSTER_COLORS[:n_clusters], ax=ax)
            ax.set_title('Key Features Distribution', fontweight='bold')
            ax.legend(title='C', fontsize=7, title_fontsize=8)
            ax.tick_params(axis='x', rotation=15)

        # 3. æƒ…æ„Ÿå¯¹æ¯”ï¼ˆæ–°å¢ï¼šsentiment_avg vs sentiment_stdï¼‰
        ax = axes[1, 0]
        if 'sentiment_avg' in self.cont_cols_final and 'sentiment_std' in self.cont_cols_final:
            for i in range(n_clusters):
                m = self.best_labels == i
                ax.scatter(
                    self.df_feat.loc[m, 'sentiment_avg'].values,
                    self.df_feat.loc[m, 'sentiment_std'].values,
                    c=CLUSTER_COLORS[i % len(CLUSTER_COLORS)],
                    label=f'C{i}', alpha=0.5, s=30
                )
            ax.set_xlabel('Sentiment Avg (æ•´ä½“å£ç¢‘)')
            ax.set_ylabel('Sentiment Std (è¯„ä»·åˆ†åŒ–ç¨‹åº¦)')
            ax.set_title('Sentiment Landscape', fontweight='bold')
            ax.legend(fontsize=8)

        # 4. ä»·æ ¼ vs é”€é‡
        ax = axes[1, 1]
        if 'log_price_per_piece' in self.cont_cols_final and 'log_sales' in self.cont_cols_final:
            for i in range(n_clusters):
                m = self.best_labels == i
                ax.scatter(
                    self.df_feat.loc[m, 'log_price_per_piece'].values,
                    self.df_feat.loc[m, 'log_sales'].values,
                    c=CLUSTER_COLORS[i % len(CLUSTER_COLORS)],
                    label=f'C{i}', alpha=0.5, s=30
                )
            ax.set_xlabel('log_price_per_piece (å•ä»·)')
            ax.set_ylabel('log_sales (é”€é‡)')
            ax.set_title('Price vs Sales by Cluster', fontweight='bold')
            ax.legend(fontsize=8)

        plt.suptitle('Cluster Analysis Dashboard v3.0', fontsize=13, fontweight='bold')
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'dashboard_v3.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()

    def _plot_feature_heatmap(self, n_clusters):
        # é€‰z-scoreç»å¯¹å€¼å˜å¼‚æœ€å¤§çš„top20ç‰¹å¾
        top_feats = self.cluster_profiles_z.abs().mean().nlargest(20).index.tolist()
        data = self.cluster_profiles_z[top_feats].T

        fig, ax = plt.subplots(figsize=(max(8, n_clusters*1.5), 12))
        sns.heatmap(data, annot=True, fmt='.2f', cmap='RdBu_r', center=0,
                   ax=ax, cbar_kws={'shrink': 0.8},
                   xticklabels=[f'C{i}' for i in range(n_clusters)],
                   annot_kws={'size': 8})
        ax.set_title('Feature Z-Score Heatmap (Top 20 Discriminative Features)',
                    fontweight='bold', fontsize=12)
        ax.tick_params(axis='y', rotation=0, labelsize=9)
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'feature_heatmap_v3.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()

    def _plot_radar(self, n_clusters):
        radar_feats = ['log_price_per_piece', 'product_rating', 'log_sales',
                       'log_reviews', 'sentiment_avg', 'positive_ratio',
                       'discount_rate', 'set_pieces']
        radar_feats = [f for f in radar_feats if f in self.cont_cols_final][:8]
        if len(radar_feats) < 3:
            return

        data = self.cluster_profiles[radar_feats].copy()
        norm = (data - data.min()) / (data.max() - data.min() + 1e-8)

        angles = np.linspace(0, 2*np.pi, len(radar_feats), endpoint=False).tolist()
        angles += angles[:1]

        fig, ax = plt.subplots(figsize=(9, 9), subplot_kw=dict(polar=True))
        for i in range(n_clusters):
            vals = norm.loc[i].tolist() + [norm.loc[i].tolist()[0]]
            ax.plot(angles, vals, 'o-', lw=2, label=f'C{i}',
                   color=CLUSTER_COLORS[i % len(CLUSTER_COLORS)], ms=5)
            ax.fill(angles, vals, alpha=0.12, color=CLUSTER_COLORS[i % len(CLUSTER_COLORS)])
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(radar_feats, fontsize=10)
        ax.set_title('Cluster Profiles Radar Chart v3.0', fontweight='bold', pad=20)
        ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1), fontsize=9)
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'radar_v3.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()

    def _plot_silhouette(self, n_clusters):
        X_eval = np.hstack([self.X_pca, self.X_cat])
        sil_vals = silhouette_samples(X_eval, self.best_labels)

        fig, ax = plt.subplots(figsize=(8, 6))
        y_lower = 10
        for i in range(n_clusters):
            cluster_sil = np.sort(sil_vals[self.best_labels == i])
            y_upper = y_lower + len(cluster_sil)
            ax.fill_betweenx(np.arange(y_lower, y_upper), 0, cluster_sil,
                            facecolor=CLUSTER_COLORS[i % len(CLUSTER_COLORS)],
                            alpha=0.7, edgecolor='white')
            ax.text(-0.05, y_lower + 0.5*len(cluster_sil), str(i), fontsize=9)
            y_lower = y_upper + 10

        avg = sil_vals.mean()
        ax.axvline(x=avg, color='red', ls='--', lw=2, label=f'Avg: {avg:.3f}')
        ax.set_xlabel('Silhouette Coefficient'); ax.set_ylabel('Cluster')
        ax.set_title('Silhouette Analysis v3.0', fontweight='bold')
        ax.legend()
        plt.tight_layout()
        plt.savefig(os.path.join(self.output_dir, 'silhouette_v3.png'),
                   dpi=150, bbox_inches='tight')
        plt.close()

    # -------------------------------------------------------------------------
    # Step 6: å‘½åä¸æŠ¥å‘Š
    # -------------------------------------------------------------------------
    def generate_report(self):
        print("\n" + "=" * 70)
        print("[Step 6] å•†ä¸šæ´å¯ŸæŠ¥å‘Š")
        print("=" * 70)

        self.cluster_names = self._auto_name_clusters()

        print("\n  ğŸ·ï¸  ç°‡å‘½å:")
        for cid, name in self.cluster_names.items():
            d = self.cluster_descriptions[cid]
            print(f"     C{cid}: {name}  ({d['size']}ä¸ª, {d['pct']:.1f}%)")

        lines = []
        lines += [
            "=" * 80,
            "        å¨åˆ€å¸‚åœºèšç±»åˆ†æ v3.0 â€” å•†ä¸šæ´å¯ŸæŠ¥å‘Š",
            "=" * 80,
            f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            f"åˆ†ææ ·æœ¬: {len(self.df_raw)} ä¸ªå•†å“",
            f"æœ‰æ•ˆç‰¹å¾: {len(self.cont_cols_final)+len(self.cat_cols_final)} ä¸ª"
            f" (è¿ç»­{len(self.cont_cols_final)}+åˆ†ç±»{len(self.cat_cols_final)})",
            f"èšç±»ç®—æ³•: {'K-Prototypes' if HAS_KMODES else 'K-Means (PCA+Cat)'}",
            f"èšç±»æ•°é‡: {self.best_k}",
            f"è½®å»“ç³»æ•°: {silhouette_score(np.hstack([self.X_pca,self.X_cat]), self.best_labels):.4f}",
        ]

        # ç¨³å®šæ€§è¯„çº§
        ari_mean, ari_std = self.stability_results.get(self.best_k, (None, None))
        if ari_mean:
            grade = 'ç¨³å®š' if ari_mean >= 0.85 else ('ä¸­ç­‰' if ari_mean >= 0.65 else 'ä¸ç¨³å®š')
            lines.append(f"èšç±»ç¨³å®šæ€§: ARI={ari_mean:.3f}Â±{ari_std:.3f} ({grade})")
            if ari_mean < 0.65:
                lines.append("âš ï¸  ç¨³å®šæ€§ä¸è¶³ï¼Œä»¥ä¸‹ç»“è®ºä»…ä¾›å‚è€ƒï¼Œä¸å»ºè®®ä½œä¸ºå¼ºå†³ç­–ä¾æ®")

        lines.append("\n" + "=" * 80)
        lines.append("                    å¸‚åœºç»†åˆ†è¯¦æƒ…")
        lines.append("=" * 80)

        for cid in range(self.best_k):
            name = self.cluster_names[cid]
            d    = self.cluster_descriptions[cid]
            z    = self.cluster_profiles_z.loc[cid]

            lines += [
                f"\n{'â”€'*80}",
                f"ã€C{cid}ã€‘ {name}   ({d['size']}ä¸ª, {d['pct']:.1f}%)",
                f"{'â”€'*80}",
                "  æ ¸å¿ƒä¼˜åŠ¿:",
            ]
            for feat, val in d['high'].items():
                lines.append(f"    + {feat}: z={val:+.2f}")
            if not d['high']:
                lines.append("    (æ— æ˜¾è‘—é«˜äºå¹³å‡)")

            lines.append("  æ”¹è¿›ç©ºé—´:")
            for feat, val in d['low'].items():
                lines.append(f"    - {feat}: z={val:+.2f}")
            if not d['low']:
                lines.append("    (æ— æ˜¾è‘—ä½äºå¹³å‡)")

            lines.append("  å»ºè®®:")
            for i, s in enumerate(self._suggest(cid, d, z.to_dict()), 1):
                lines.append(f"    {i}. {s}")

        lines += [
            "\n" + "=" * 80,
            "                    æ•´ä½“å¸‚åœºæ´å¯Ÿ",
            "=" * 80,
        ]
        for insight in self._overall_insights():
            lines.append(f"  {insight}")

        lines += ["", "=" * 80, "                      æŠ¥å‘Šç»“æŸ", "=" * 80]

        report = "\n".join(lines)
        path = os.path.join(self.output_dir, 'business_report_v3.txt')
        with open(path, 'w', encoding='utf-8') as f:
            f.write(report)
        print(report)
        print(f"\n  ğŸ“„ æŠ¥å‘Šå·²ä¿å­˜: {path}")
        return self

    def _auto_name_clusters(self):
        names = {}
        for cid in range(self.best_k):
            z    = self.cluster_profiles_z.loc[cid]
            d    = self.cluster_descriptions[cid]
            tags = []

            # è§„æ¨¡
            if d['pct'] > 35:   tags.append('ä¸»æµ')
            elif d['pct'] < 6:  tags.append('ç»†åˆ†')

            # ä»·æ ¼
            pz = z.get('log_price_per_piece', 0)
            if   pz >  0.6: tags.append('é«˜ä»·')
            elif pz >  0.2: tags.append('ä¸­é«˜ä»·')
            elif pz < -0.6: tags.append('ä½ä»·')
            elif pz < -0.2: tags.append('ä¸­ä½ä»·')

            # å¥—è£…
            sz = self.cluster_profiles.loc[cid].get('set_pieces', 0)
            if   sz > 3:   tags.append('å¥—è£…')
            elif sz < 1.5: tags.append('å•å“')

            # å£ç¢‘
            sa = z.get('sentiment_avg', z.get('positive_ratio', 0))
            if   sa >  1.5: tags.append('å£ç¢‘ä½³')
            elif sa < -2.0: tags.append('å£ç¢‘å·®')

            # é”€é‡
            sv = z.get('log_sales', 0)
            if   sv >  0.8: tags.append('ç•…é”€')
            elif sv < -1.0: tags.append('æ»é”€')

            # æè´¨
            if self.cluster_profiles.loc[cid].get('is_damascus', 0) > 0.15:
                tags.append('å¤§é©¬å£«é©')
            elif self.cluster_profiles.loc[cid].get('is_japanese_steel', 0) > 0.12:
                tags.append('æ—¥ç³»')
            elif self.cluster_profiles.loc[cid].get('is_german_steel', 0) > 0.12:
                tags.append('å¾·ç³»')

            names[cid] = '-'.join(tags[:3]) if tags else f'ç»†åˆ†å¸‚åœº{cid}'
        return names

    def _suggest(self, cid, d, z):
        sug = []
        if z.get('log_price_per_piece', 0) > 0.5:
            sug.append("é«˜å•ä»·å®šä½ï¼Œå¼ºåŒ–å“è´¨å†…å®¹å’Œå“ç‰Œæº¢ä»·å™äº‹")
        if z.get('log_price_per_piece', 0) < -0.5:
            sug.append("ä»·æ ¼æ•æ„Ÿå¸‚åœºï¼Œè€ƒè™‘å¥—è£…/æ†ç»‘é”€å”®æå‡å®¢å•ä»·")
        if 'sentiment_avg' in d['low'] or 'positive_ratio' in d['low']:
            sug.append("å£ç¢‘ä¸‹æ»‘ï¼Œä¼˜å…ˆæ”¹å–„äº§å“è´¨é‡ï¼Œå›åº”è´Ÿé¢è¯„ä»·")
        if 'sentiment_avg' in d['high'] or 'positive_ratio' in d['high']:
            sug.append("å£ç¢‘æ˜¯æ ¸å¿ƒä¼˜åŠ¿ï¼Œé¼“åŠ±æ™’å•ï¼Œå¼ºåŒ–ç¤¾ä¼šè¯æ˜")
        if 'log_sales' in d['high']:
            sug.append("çƒ­é”€å“ç±»ï¼Œæµ‹è¯•ä»·æ ¼å¼¹æ€§ç©ºé—´")
        if 'log_sales' in d['low'] or 'log_reviews' in d['low']:
            sug.append("æ›å…‰ä¸è¶³ï¼ŒåŠ å¼ºå¹¿å‘ŠæŠ•æ”¾å’Œå…³é”®è¯å¸ƒå±€")
        if z.get('set_pieces', 0) > 0.5:
            sug.append("å¥—è£…å¸‚åœºï¼Œä¼˜åŒ–åˆ€ç»„æ­é…å’Œç¤¼ç›’åŒ…è£…")
        if z.get('set_pieces', 0) < -0.3:
            sug.append("å•å“èµ›é“ï¼Œçªå‡ºä¸“ä¸šæ€§èƒ½å’Œä½¿ç”¨åœºæ™¯")
        if not sug:
            sug.append("ç»´æŒç°æœ‰ç­–ç•¥ï¼ŒæŒç»­ç›‘æµ‹ç«å“åŠ¨æ€")
        return sug[:4]

    def _overall_insights(self):
        insights = []
        sizes = pd.Series(self.best_labels).value_counts()
        top2  = sizes.nlargest(2).sum() / len(self.best_labels)
        insights.append(f"å¸‚åœºé›†ä¸­åº¦ï¼šå‰2ç°‡å  {top2*100:.1f}%")

        if 'product_rating' in self.cont_cols_final:
            avg_r = self.df_feat['product_rating'].mean()
            insights.append(f"å¹³å‡è¯„åˆ†ï¼š{avg_r:.2f} åˆ†")

        if 'positive_ratio' in self.cont_cols_final:
            avg_p = self.df_feat['positive_ratio'].mean()
            insights.append(f"æ•´ä½“æ­£å‘æƒ…æ„Ÿæ¯”ï¼š{avg_p:.3f}")

        if 'sentiment_std' in self.cont_cols_final:
            avg_std = self.df_feat['sentiment_std'].mean()
            insights.append(
                f"è¯„ä»·åˆ†åŒ–æŒ‡æ•°ï¼ˆå‡å€¼ï¼‰ï¼š{avg_std:.3f} "
                f"({'è¯„ä»·è¾ƒä¸€è‡´' if avg_std < 0.2 else 'è¯„ä»·è¾ƒåˆ†åŒ–'})"
            )

        if 'is_fba' in self.cat_cols_final:
            fba_r = self.df_feat['is_fba'].mean()
            insights.append(f"FBAå æ¯”ï¼š{fba_r*100:.1f}%")

        if 'set_pieces' in self.cont_cols_final:
            set_r = (self.df_feat['set_pieces'] > 1).mean()
            insights.append(f"å¥—è£…å•†å“å æ¯”ï¼š{set_r*100:.1f}%")

        return insights

    # -------------------------------------------------------------------------
    # Step 7: ä¿å­˜
    # -------------------------------------------------------------------------
    def save_results(self):
        print("\n" + "=" * 70)
        print("[Step 7] ä¿å­˜ç»“æœæ–‡ä»¶")
        print("=" * 70)

        files = {}

        # å¸¦æ ‡ç­¾çš„å®Œæ•´æ•°æ®
        out = self.df_raw.copy()
        out['cluster']      = self.best_labels
        out['cluster_name'] = pd.Series(self.best_labels).map(self.cluster_names)
        out['tsne_x']       = self.X_tsne[:, 0]
        out['tsne_y']       = self.X_tsne[:, 1]
        out['pca_x']        = self.X_pca[:, 0]
        out['pca_y']        = self.X_pca[:, 1]
        p = os.path.join(self.output_dir, 'clustered_products_v3.csv')
        out.to_csv(p, index=False, encoding='utf-8-sig'); files['äº§å“èšç±»ç»“æœ'] = p

        # ç°‡ç”»åƒ
        p = os.path.join(self.output_dir, 'cluster_profiles_v3.csv')
        self.cluster_profiles.to_csv(p, encoding='utf-8-sig'); files['ç°‡ç‰¹å¾å‡å€¼'] = p

        p = os.path.join(self.output_dir, 'cluster_profiles_zscore_v3.csv')
        self.cluster_profiles_z.to_csv(p, encoding='utf-8-sig'); files['ç°‡ç‰¹å¾Zåˆ†'] = p

        # Kå€¼è¯„ä¼°æŒ‡æ ‡
        p = os.path.join(self.output_dir, 'clustering_metrics_v3.csv')
        pd.DataFrame(self.metrics).to_csv(p, index=False, encoding='utf-8-sig')
        files['Kå€¼è¯„ä¼°'] = p

        # Bootstrapç¨³å®šæ€§
        stab_rows = [{'k': k, 'ari_mean': v[0], 'ari_std': v[1]}
                     for k, v in self.stability_results.items()]
        p = os.path.join(self.output_dir, 'stability_results_v3.csv')
        pd.DataFrame(stab_rows).to_csv(p, index=False, encoding='utf-8-sig')
        files['ç¨³å®šæ€§æ£€éªŒ'] = p

        # ç°‡åæ˜ å°„
        rows = [{'cluster': k, 'name': v,
                 'size': self.cluster_descriptions[k]['size'],
                 'pct':  self.cluster_descriptions[k]['pct']}
                for k, v in self.cluster_names.items()]
        p = os.path.join(self.output_dir, 'cluster_names_v3.csv')
        pd.DataFrame(rows).to_csv(p, index=False, encoding='utf-8-sig')
        files['ç°‡å‘½å'] = p

        for desc, path in files.items():
            print(f"  âœ… {desc}: {path}")

        print(f"\n  ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}/")
        return self

    # -------------------------------------------------------------------------
    # ä¸€é”®è¿è¡Œ
    # -------------------------------------------------------------------------
    def run(self, k_range=(2, 9), final_k=None,
            n_gap_refs=15, n_bootstrap=20):
        """
        å®Œæ•´æµæ°´çº¿å…¥å£

        Args:
            k_range:      Kå€¼æœç´¢èŒƒå›´
            final_k:      å¼ºåˆ¶æŒ‡å®šKå€¼ï¼ˆNone=è‡ªåŠ¨é€‰æ‹©ï¼‰
            n_gap_refs:   Gap Statistic å‚è€ƒæ•°æ®é›†æ•°é‡
            n_bootstrap:  Bootstrap é‡é‡‡æ ·æ¬¡æ•°
        """
        self.preprocess()
        self.reduce_dimensions()
        self.find_optimal_k(k_range=k_range,
                            n_gap_refs=n_gap_refs,
                            n_bootstrap=n_bootstrap)
        if final_k is not None:
            print(f"\n  ğŸ“Œ ç”¨æˆ·æŒ‡å®š K = {final_k} (è‡ªåŠ¨æ¨è: {self.best_k})")
            self.best_k = final_k

        self.run_clustering()
        self.analyze_clusters()
        self.generate_report()
        self.save_results()

        print("\n" + "=" * 70)
        print("  âœ… v3.0 åˆ†æå®Œæˆï¼")
        sil = silhouette_score(np.hstack([self.X_pca, self.X_cat]), self.best_labels)
        ari_mean = self.stability_results.get(self.best_k, (None,))[0]
        ari_str = f"{ari_mean:.3f}" if ari_mean is not None else "N/A"
        print(f"     K={self.best_k} | è½®å»“ç³»æ•°={sil:.4f} | ç¨³å®šæ€§ARI={ari_str}")
        print("=" * 70)
        return self


# ============================================================================
# å…¥å£
# ============================================================================
if __name__ == '__main__':
    pipeline = ClusteringPipelineV3(
        data_path  = 'clustering_features_only.csv',
        output_dir = 'clustering_results_v3'
    ).run(
        k_range     = (2, 9),   # Kå€¼æœç´¢èŒƒå›´
        final_k     = 3,     # None=è‡ªåŠ¨; å¡«æ•°å­—=å¼ºåˆ¶
        n_gap_refs  = 15,       # Gap Statistic å‚è€ƒç»„æ•°ï¼ˆè¶Šå¤§è¶Šå‡†ï¼Œè¶Šæ…¢ï¼‰
        n_bootstrap = 20,       # Bootstrap è½®æ•°ï¼ˆâ‰¥20 ç»“æœç¨³å®šï¼‰
    )