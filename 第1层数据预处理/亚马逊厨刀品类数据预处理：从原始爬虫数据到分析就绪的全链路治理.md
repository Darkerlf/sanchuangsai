# 亚马逊厨刀品类数据预处理：从原始爬虫数据到分析就绪的全链路治理

## 一、子项目定位：数据治理的基石

在"数智驱动下的'中国好刀'"全链路架构中，**第一层数据预处理**承担着至关重要的"数据净化器"角色。该项目将分散、冗余、非结构化的爬虫原始数据，转化为高质量、标准化、可分析的商业数据资产，为后续七层架构提供坚实基础。具体而言，本层实现了三大核心价值：

- **数据融合**：将多批次采集的碎片化数据（不同时间抓取的商品/评论表）整合为统一视图
- **质量提升**：通过去重、清洗、结构化处理，将原始数据质量从"可用"提升至"可信"
- **维度扩展**：构建商品-评论联动的事实表与聚合表，为NLP、预测建模、聚类分析提供多维特征

这种"一进多出"的架构设计（输入原始爬虫数据，输出7+种分析就绪表），使得本层成为连接原始数据与智能分析的**关键枢纽**，直接影响后续所有分析的可靠性与深度。

## 二、数据流全景：从原始采集到分析就绪

### 1. 数据输入：爬虫产出的原始资产
项目处理两类核心数据源：
- **商品表**：包含`products-1-28.csv`、`products.csv`、`products-1-10.csv`三个批次，记录了约946个厨刀ASIN的基础信息、价格、排名等字段
- **评论表**：包含`reviews-1-28.csv`、`reviews-1-10.csv`两个批次，收录了约2218条用户评论，涵盖评分、文本、购买验证等维度

这些原始数据存在显著挑战：同一件商品在不同批次中可能重复出现（信息更新不一致），评论表结构不统一（如缺失brand列），字段格式混乱（如价格含$符号，图片存储为JSON字符串）。

### 2. 处理流程：四步走战略
整个预处理流程采用**渐进式清洗策略**，通过四个Python脚本串联执行，形成清晰的数据流水线：

**Step 1：跨批次合并与主键去重**  
```python
# 商品表合并.py 与 商品评论表合并.py 的核心逻辑
combined_df = pd.concat(dfs, ignore_index=True)  # 合并多文件
combined_df = combined_df.sort_values('updated_at', ascending=False)  # 按时间倒序
dedup_df = combined_df.drop_duplicates(subset='asin', keep='first')  # 保留最新记录
```
这一阶段解决了**数据冗余**问题：商品按`asin`去重（保留`updated_at`最新的记录），评论按`review_id`去重（保留`created_at`最新的记录）。特别针对评论表，脚本智能补全缺失字段（如为无brand列的批次填充空值），确保结构统一。

**Step 2：深度清洗与特征工程**  
```python
# 数据预处理.py 的核心转换
prod["price_num"] = prod["price"].apply(parse_usd)  # 价格字段解析
prod["stock_left"] = prod["availability"].apply(parse_stock_left)  # 库存结构化
rev["review_text_clean"] = rev["review_text_raw"].apply(clean_text_basic)  # 评论文本清洗
```
这一阶段完成**8大类转换**：
- **货币解析**：将`$1,299.99`转换为浮点数1299.99，计算折扣率
- **JSON扁平化**：解析`images`、`sub_category_ranks`、`bullet_points`等嵌套结构
- **文本清理**：移除HTML标签、标准化空格、处理特殊字符
- **数值提取**：从`"Only 3 left in stock"`提取数字3
- **字段重命名**：区分`product_rating`与`review_rating`避免语义混淆
- **特征衍生**：计算文本大写比例、标点密度等NLP特征
- **时序对齐**：标准化日期格式，为后续时间序列分析铺路
- **缺失值策略**：对商品评分等关键字段采用业务合理填充（如无评分视为0分）

**Step 3：评论文本深度净化**  
```python
# 数据进一步清洗.py 的核心逻辑
def clean_video_garbage(text):
    garbage_marker = "This is a modal window."
    if garbage_marker in text:
        return text.split(garbage_marker)[-1].strip()  # 仅保留标志后的真实评论
```
针对Amazon评论特有的**视频播放器垃圾文本**（如"The video showcases... This is a modal window."），本脚本精准识别并剥离干扰内容，大幅提升NLP分析质量。同时提供**停用词过滤版本**，为词云与主题建模准备纯净文本。

**Step 4：主分析表构建**  
```python
# 主分析表生成.py 的聚合逻辑
review_stats = reviews.groupby('asin').agg(
    review_count=('review_id', 'count'),
    avg_rating=('rating', 'mean'),
    positive_rate=('rating', lambda x: (x >= 4).mean())
).reset_index()
master_df = products.merge(review_stats, on='asin', how='left')
```
最后，将商品维度与评论洞察**横向融合**，生成`master_analysis_table.csv`——一张包含价格、品牌、评分、评论情感等20+关键指标的全景表，满足商业分析的"一表通览"需求。

### 3. 数据输出：多维度分析资产
预处理层最终产出**7类核心数据资产**，分属三个层次：
- **基础层**（原始数据净化版）：
  - `products_clean.csv`：946条商品记录，40+结构化字段
  - `reviews_clean.csv`：2218条评论记录，30+清洗后特征
- **关联层**（跨表联动）：
  - `fact_review_enriched.csv`：评论事实表（每条评论+商品维度）
  - `product_images.csv`/`product_subcat_ranks.csv`：商品明细拆表
- **聚合层**（分析就绪）：
  - `agg_product.csv`：ASIN级聚合（评论统计、情感指标）
  - `agg_product_week.csv`：ASIN×周级聚合（时间趋势分析）
  - `master_analysis_table.csv`：商业决策主表

## 三、技术创新点：超越常规清洗

本层预处理在三个方面实现了突破性设计，显著提升了数据价值：

### 1. 业务语义深度理解
- **价格体系重构**：区分`list_price`（标价）、`unit_price`（单价）、`price_num`（实付价），精准计算折扣率
- **库存信号解码**：将文本描述`availability`量化为`in_stock_flag`（是否在售）与`stock_left`（剩余库存）
- **销量代理指标**：从`"Bought in past month: 100+"`提取`bought_count_number_clean`，构建销量代理变量

### 2. 采样偏差控制机制
- **抓取上限标记**：自动识别评论抓取达到平台上限的商品（如固定10条/100条），生成`scrape_cap_10_flag`/`scrape_cap_100_flag`
- **数据质量看板**：统计`sample_review_n`（样本评论数）与`verified_ratio`（真实购买比例），标注数据可信度

### 3. 多模态特征融合
- **文本-结构化联动**：在评论表中同时保留原始文本、清洗文本、去停用词文本，满足不同NLP任务需求
- **图像元数据分析**：从`images` JSON提取`image_count`、`main_image`，量化商品视觉呈现质量
- **变体维度拆解**：将`variant`字段（如"Color: Black; Size: 8 Inch"）拆分为`variant_color`/`variant_size`等结构化维度

## 四、项目实践启示

### 1. 可复现的工程规范
- **目录结构清晰**：原始数据/中间产物/最终输出分层存储
- **执行顺序明确**：提供标准化运行命令（`python 商品表合并.py`→`python 数据预处理.py`）
- **错误处理完备**：文件存在性检查、字段缺失补全、异常数据标记

### 2. 业务风险透明化
- **数据局限性声明**：明确标注`bought_count_number_clean`非官方销量，可能受抓取上限影响
- **口径统一提醒**：强调NLP分析应使用`reviews_cleaned_v2.csv`而非原始清洗表
- **偏差解释准备**：输出`scrape_cap_flag`字段，为后续分析结论提供数据质量注释

## 五、总结：数据治理的价值升华

本层预处理工作远不止于"技术性清洗"，而是实现了**数据资产的价值跃迁**：
- **从碎片到全景**：将分散的爬虫数据整合为商品-评论联动的商业全景
- **从噪声到信号**：通过20+项清洗规则，提取价格、库存、评论情感等高质量信号
- **从原始到智能**：构建时序聚合、文本特征、图像元数据，为上层AI模型铺平道路

正如项目文档精准概括："本层把爬虫产出的多批次原始表合并→去重→结构化清洗→生成分析用主表/事实表，为后续NLP、建模、聚类、知识图谱提供'可直接读取'的干净数据"。这种**以终为始的数据治理思维**，正是确保"八层架构"能够从数据源头到商业决策全程贯通的关键保障。

> **后续价值延伸**：当您提供输入/输出数据样本后，我将进一步分析数据分布特征、清洗前后对比效果，以及如何基于这些高质量数据构建更具说服力的厨刀品类竞争策略。这些洞察将直接支撑方案中第三层（预测建模）与第七层（AIGC生成）的核心创新点，打造真正"用数据刀锋切开海外市场"的三创赛标杆作品。