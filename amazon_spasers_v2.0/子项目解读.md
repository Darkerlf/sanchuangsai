# amazon_spasers_v2.0｜Amazon 数据采集（爬虫）子项目解读

> 说明：本文中出现的相对路径，默认以项目根目录 `E:\PycharmProjects\三创赛` 为基准。

## 1. 子项目定位（在全链路中的作用）

本子项目负责**从 Amazon 前台页面采集原始数据**，为后续的「数据预处理 → NLP 处理 → 预测建模/聚类/知识图谱」提供数据来源。

- 上游：无（从 Amazon 页面抓取）
- 下游：
  - `第1层数据预处理/`：把不同批次的抓取结果合并、去重、清洗，形成统一分析表
  - `知识图谱分析与关联层/`：可选读取 `data/amazon_data.db` 的 `search_results` 表补充搜索端信息

---

## 2. 解决的问题与核心思路

**问题**：比赛场景中，平台公开数据分散在「搜索列表 / 商品详情 / 评论页」，且存在反爬限制、分页、断点续抓等工程问题。

**思路**：采用 Selenium 驱动真实浏览器，按“三阶段流程”完成采集，并通过存储层实现断点续抓与多格式导出。

---

## 3. 输入数据（你需要提供什么）

主要输入来自运行参数 / 配置（非静态数据文件）：

- 搜索关键词（如 `kitchen knife`, `McCook knife`）
- 品牌过滤条件（如 `imarku`、`McCook`）
- 指定 ASIN 列表（可选）
- 登录态（首次运行需要手动登录 Amazon 账号）

配置位置：`amazon_spasers_v2.0/config.py`

---

## 4. 关键技术点（比赛可写的“技术亮点”）

- **Web 自动化采集**：Python + Selenium + Chrome（真实浏览器渲染，适配动态页面）
- **反爬/稳定性**：
  - 随机延迟、User-Agent 轮换
  - 浏览器重启分批抓取
  - 断点续传（避免中断后重跑）
  - 验证码/风控页面（“狗狗页”）检测与人工介入
- **模块化工程结构**：
  - `scrapers/` 负责抓取流程
  - `parsers/` 负责 DOM/页面结构解析
  - `storage/` 负责落库/导出（SQLite/JSON/CSV 等）

---

## 5. 输出数据与文件（落地产物清单）

### 5.1 结构化数据（核心）

输出目录：`amazon_spasers_v2.0/data/`

- `amazon_data.db`：SQLite 数据库（核心落库）
  - `search_results`：搜索结果（asin/brand/title/price/rating/search_rank 等）
  - `products`：商品详情（asin/title/price/rating/bsr_rank/bought_count 等）
  - `reviews`：评论（asin/rating/content/verified_purchase/helpful_votes/author/date 等）
- `search_results*.json`：搜索结果抓取快照（带时间戳版本）
- `products*.json`：商品详情抓取快照
- `reviews*.json`：评论抓取快照

> 项目书写法：把这一层描述成“数据采集层”，强调**可重复抓取、可追溯快照、结构化落库**。

### 5.2 调试证据（可用于“工程能力展示”）

输出目录：`amazon_spasers_v2.0/debug/`

- `login_success_review.png`、`login_review_check.png`：登录/登录态验证截图
- `review_page_*.png`、`test_bought_*.png`：页面解析与字段抓取验证截图

> 项目书写法：在“数据质量控制/反爬策略”部分插入 1 张调试截图，说明你们做了**可视化验收**，不是黑盒爬取。

---

## 6. 在比赛项目书中怎么描述（可直接引用）

你可以写成如下模板（建议按“数据采集 → 落库 → 可追溯”组织）：

1. **数据采集对象**：Amazon 美国站厨刀类目商品的搜索结果、商品详情信息与用户评论。
2. **采集技术路线**：采用 Selenium 驱动真实浏览器，按“搜索→详情→评论”三阶段抓取；引入随机延迟与 UA 轮换以降低反爬风险；支持断点续抓提升工程稳定性。
3. **数据落地与可追溯**：结果统一写入 SQLite（`amazon_data.db`），并同步导出 JSON 快照（带时间戳）以便回溯与复现。
4. **数据质量控制**：通过调试截图与日志记录对关键字段（价格、评分、BSR、bought_count、评论内容）进行抽样验证。

---

## 7. 复现运行方式（命令级）

在 `amazon_spasers_v2.0/` 目录下：

```bash
pip install -r requirements.txt

# 完整流程（搜索 + 商品 + 评论 + 导出）
python main.py all

# 分阶段按品牌抓取
python main.py brand --brand imarku --stage search
python main.py brand --brand imarku --stage products
python main.py brand --brand imarku --stage reviews -n 50
```

---

## 8. 注意事项（重要）

- **合规**：仅用于学习研究，需遵守 Amazon 服务条款与相关规范（项目中已写免责声明）
- **登录与验证码**：首次抓取要人工登录；遇到验证码需人工处理后继续
- **与后续层衔接**：若后续预处理使用的是 `products-*.csv / reviews-*.csv`，需要先把 DB/JSON 进一步导出为对应 CSV（可通过本项目的 export 功能或自写导出脚本实现）
