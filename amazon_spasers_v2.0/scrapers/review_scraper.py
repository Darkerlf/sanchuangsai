"""
è¯„è®ºçˆ¬è™« - æŠ“å– Amazon è¯„è®ºé¡µé¢ (ç»ˆæä¿®æ­£ç‰ˆ)
"""

import time
import random
import logging
from typing import List

from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException

from browser import BrowserManager
from parsers import ReviewParser
from parsers.review_parser import Review
from config import config

logger = logging.getLogger(__name__)


class ReviewScraper:
    """è¯„è®ºçˆ¬è™«"""

    def __init__(self, browser: BrowserManager):
        self.browser = browser
        self.parser = ReviewParser()

    def scrape(self, asin: str, max_pages: int = None, brand: str = "") -> List[Review]:
        """
        æŠ“å–å•†å“è¯„è®º
        ç‰¹æ€§ï¼š
        1. æ¨¡æ‹ŸçœŸäººç‚¹å‡»ç¿»é¡µ (è§£å†³é‡å®šå‘å›é¦–é¡µé—®é¢˜)
        2. ç‹—ç‹—é¡µ(Dog Page)æ£€æµ‹ä¸è‡ªåŠ¨ç­‰å¾… (è§£å†³åçˆ¬æ‹¦æˆª)
        """
        max_pages = max_pages or config.REVIEWS_MAX_PAGES
        all_reviews = []
        seen_ids = set()

        logger.info(f"å¼€å§‹æŠ“å–è¯„è®º: {asin} (å“ç‰Œ: {brand})")

        # 1. åˆå§‹è®¿é—®ç¬¬ 1 é¡µ
        # ä½¿ç”¨æ˜¾å¼å‚æ•°æœ‰åŠ©äºå»ºç«‹æ­£ç¡®çš„ Session
        url = f"https://www.amazon.com/product-reviews/{asin}/ref=cm_cr_dp_d_show_all_btm?ie=UTF8&reviewerType=all_reviews&sortBy=recent"

        driver = self.browser.get_driver()
        try:
            driver.get(url)
            self.browser.wait_for_page_load()
        except Exception as e:
            logger.error(f"åˆå§‹é¡µé¢åŠ è½½å¤±è´¥: {e}")
            return []

        for page in range(1, max_pages + 1):
            try:
                logger.info(f"æ­£åœ¨å¤„ç†ç¬¬ {page} é¡µ...")

                # ==========================================
                # ğŸ›‘ æ ¸å¿ƒé˜²æŠ¤ï¼šæ£€æµ‹äºšé©¬é€Šâ€œç‹—ç‹—é¡µâ€ (åçˆ¬æ‹¦æˆª)
                # ==========================================
                title = driver.title.lower()
                if "sorry" in title or "server busy" in title or "robot check" in title:
                    logger.warning(f"ğŸ¶ æ±ªæ±ªï¼æ£€æµ‹åˆ° Amazon ç‹—ç‹—é¡µ (åçˆ¬æ‹¦æˆª) - ASIN: {asin}")
                    logger.info("â³ è§¦å‘ç†”æ–­ä¿æŠ¤ï¼šæš‚åœ 60 ç§’ç­‰å¾…è§£å°...")
                    time.sleep(60)

                    try:
                        logger.info("ğŸ”„ å°è¯•åˆ·æ–°é¡µé¢...")
                        driver.refresh()
                        self.browser.wait_for_page_load()

                        # å†æ¬¡æ£€æŸ¥æ˜¯å¦è§£é™¤
                        if "sorry" in driver.title.lower():
                            logger.error("âŒ åˆ·æ–°æ— æ•ˆï¼Œä»ç„¶è¢«æ‹¦æˆªã€‚æ”¾å¼ƒå½“å‰å•†å“å‰©ä½™é¡µé¢ã€‚")
                            break
                    except Exception:
                        break
                # ==========================================

                # --- éšæœºè¡Œä¸ºæ¨¡æ‹Ÿ ---
                time.sleep(random.uniform(2.5, 5.0))  # ç¨å¾®è°ƒå¤§ç­‰å¾…æ—¶é—´
                self.browser.scroll_page()

                # --- éªŒè¯ç æ£€æŸ¥ ---
                if self.browser.check_captcha():
                    logger.warning("é‡åˆ°éªŒè¯ç ï¼Œè¯·æ‰‹åŠ¨å¤„ç†...")
                    # å¯ä»¥åœ¨è¿™é‡ŒåŠ  input() é˜»å¡ï¼Œæˆ–è€…ç›´æ¥è·³è¿‡
                    time.sleep(5)
                    if self.browser.check_captcha():
                         logger.error("éªŒè¯ç æœªé€šè¿‡ï¼Œè·³è¿‡")
                         break

                # --- è§£æå½“å‰é¡µ ---
                html = driver.page_source
                reviews = self.parser.parse(html, asin=asin)

                # æ ‡è®°å“ç‰Œ
                for review in reviews:
                    review.brand = brand

                # --- å»é‡é€»è¾‘ ---
                new_reviews = []
                for r in reviews:
                    if r.review_id not in seen_ids:
                        seen_ids.add(r.review_id)
                        new_reviews.append(r)

                # --- âš ï¸ é˜²æ­¢æ— é™é‡å®šå‘å›ç¬¬ 1 é¡µçš„ä¿æŠ¤æœºåˆ¶ ---
                # å¦‚æœä¸æ˜¯ç¬¬ 1 é¡µï¼Œå´æ‰¾åˆ°äº†è¯„è®ºï¼Œä½†å…¨æ˜¯æ—§çš„ï¼Œè¯´æ˜äºšé©¬é€ŠæŠŠæˆ‘ä»¬è¸¢å›äº†ç¬¬ 1 é¡µ
                if page > 1 and len(reviews) > 0 and len(new_reviews) == 0:
                    logger.warning(f"âš ï¸ ç¬¬ {page} é¡µæ£€æµ‹åˆ°é‡å¤å†…å®¹ï¼ˆAmazon é‡å®šå‘å›é¦–é¡µï¼‰ï¼Œåœæ­¢æŠ“å–ã€‚")
                    break

                # å¦‚æœé¡µé¢æœ¬èº«å°±æ²¡æœ‰è¯„è®ºï¼ˆè§£æå‡º0æ¡ï¼‰ï¼Œä¸”ä¸æ˜¯ç¬¬1é¡µï¼Œè¯´æ˜åˆ°åº•äº†
                if len(reviews) == 0 and page > 1:
                    logger.info("æ²¡æœ‰è¯»å–åˆ°è¯„è®ºï¼Œå¯èƒ½å·²åˆ°è¾¾æœ«é¡µã€‚")
                    break

                logger.info(f"âœ… ç¬¬ {page} é¡µ: æˆåŠŸæå– {len(new_reviews)} æ¡æ–°è¯„è®º")
                all_reviews.extend(new_reviews)

                # --- ç¿»é¡µé€»è¾‘ (ä½¿ç”¨ JavaScript ç‚¹å‡») ---
                if page < max_pages:
                    try:
                        # 1. å¯»æ‰¾â€œä¸‹ä¸€é¡µâ€æŒ‰é’® (ç²¾å‡†å®šä½ li.a-last > a)
                        next_btn = WebDriverWait(driver, 5).until(
                            EC.presence_of_element_located((By.CSS_SELECTOR, "li.a-last a"))
                        )

                        # 2. è·å– URL ä»…ç”¨äºæ—¥å¿—ï¼Œä¸ç”¨äºè·³è½¬
                        next_url = next_btn.get_attribute("href")
                        logger.debug(f"å‡†å¤‡è·³è½¬ä¸‹ä¸€é¡µ... Target: {next_url}")

                        # 3. ã€å…³é”®ã€‘ä½¿ç”¨ JS ç‚¹å‡»ï¼Œä¿ç•™ Refererï¼Œæ¨¡æ‹ŸçœŸå®ç”¨æˆ·
                        driver.execute_script("arguments[0].click();", next_btn)

                        # 4. ã€å…³é”®ã€‘ç­‰å¾… URL å‘ç”Ÿå˜åŒ–ï¼Œç¡®ä¿ç¿»é¡µæˆåŠŸ
                        # æˆ‘ä»¬ç­‰å¾… URL ä¸­å‡ºç° pageNumber={page+1}
                        try:
                            WebDriverWait(driver, 10).until(
                                lambda d: f"pageNumber={page + 1}" in d.current_url
                                or (next_url and next_url in d.current_url)
                            )
                        except TimeoutException:
                            logger.warning(f"â³ ç¬¬ {page} é¡µç‚¹å‡»å URL æœªåŠæ—¶å˜åŒ–ï¼Œå¯èƒ½åŠ è½½è¾ƒæ…¢æˆ–å·²è¢«é‡å®šå‘")

                        self.browser.wait_for_page_load()

                    except (NoSuchElementException, TimeoutException):
                        logger.info("ğŸš« æ²¡æœ‰â€œä¸‹ä¸€é¡µâ€æŒ‰é’®äº†ï¼ŒæŠ“å–ç»“æŸã€‚")
                        break
                    except Exception as e:
                        logger.error(f"âŒ ç¿»é¡µæ“ä½œå¤±è´¥: {e}")
                        break

            except Exception as e:
                logger.error(f"ç¬¬ {page} é¡µå‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
                break

        logger.info(f"ğŸ‰ è¯„è®ºæŠ“å–å®Œæˆ: {asin}, å…± {len(all_reviews)} æ¡")
        return all_reviews

    def scrape_batch(self, asins: List[str], save_callback=None, brand: str = "") -> List[Review]:
        """
        æ‰¹é‡æŠ“å–è¯„è®º
        """
        all_reviews = []

        total = len(asins)
        for idx, asin in enumerate(asins, 1):
            logger.info(f"ğŸ‘‰ æ­£åœ¨å¤„ç†ç¬¬ {idx}/{total} ä¸ªå•†å“: {asin}")

            reviews = self.scrape(asin, brand=brand)
            all_reviews.extend(reviews)

            # å®šæœŸä¿å­˜
            if save_callback:
                save_callback(reviews)  # æ¯æ¬¡çˆ¬å®Œä¸€ä¸ªå•†å“å°±ä¿å­˜ä¸€æ¬¡ï¼Œæ›´å®‰å…¨

            # å•†å“ä¹‹é—´çš„å¤§å»¶è¿Ÿï¼Œé˜²æ­¢å°å·
            if idx < total:
                sleep_time = random.uniform(5.0, 10.0)
                logger.info(f"ğŸ’¤ å•†å“é—´ä¼‘æ¯ {sleep_time:.1f} ç§’...")
                time.sleep(sleep_time)

        logger.info(f"æ‰¹é‡è¯„è®ºæŠ“å–å®Œæˆï¼Œå…± {len(all_reviews)} æ¡")
        return all_reviews